{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c350f5a0-bca6-42e0-a4a7-5747bce13a57",
   "metadata": {},
   "source": [
    "# Segment Anything by Meta\n",
    "\n",
    "**Author:** Burak Dogan  \n",
    "\n",
    "Following code snippets and some of the technical explanations are taken from Segment Anything Git Hub repo and demo notebooks. Tailored the the need of the project.\n",
    "  \n",
    "\n",
    "[Segment Anything repo](https://github.com/facebookresearch/segment-anything)\n",
    "\n",
    "Segment Anything model is used for masking and cropping the images.\n",
    "\n",
    "Depending on the need there are three possible models to use\n",
    "\n",
    "1. Vit-H (Huge) : Largest and most complex\n",
    "2. Vit-L (Large): Smaller than large model and less complex \n",
    "3. Vit-B (Base) : Base model, least complex and smaller than the rest\n",
    "\n",
    "Use of Vit-B will be enough for this project.\n",
    "\n",
    "### Enviroment Setup\n",
    "\n",
    "Using Anaconda.\n",
    "\n",
    "Created new environment just for SAM model.\n",
    "\n",
    "To install correct version and compute platform (GPU or CPU) of Pytorch, visit [Pytorch](https://pytorch.org/get-started/locally/#anaconda)\n",
    "\n",
    "Currently using:\n",
    "- Build: Pytorch 2.3.0\n",
    "- Windows OS\n",
    "- Package: Conda\n",
    "- Language: Python\n",
    "- Compute Platform: CUDA 11.8\n",
    "\n",
    "Need to update the CUDA version on the computer if necessary (currently 12.7.33)\n",
    "\n",
    "Command to use for this environment : `conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia`\n",
    "\n",
    "Other dependencies to install:\n",
    "\n",
    "Also the model can be installed through conda [here](https://anaconda.org/conda-forge/segment-anything) or repo can be cloned from Github.\n",
    "\n",
    "- `conda install conda-forge::segment-anything`\n",
    "- `conda install numpy`\n",
    "- `conda install matplotlib`\n",
    "- `conda install -c conda-forge opencv` - Open CV is used for visualizing the images if needed.\n",
    "  \n",
    "\n",
    "Segment Anything GitHub repo has pre-trained model checkpoints. \n",
    "  \n",
    "[Vit-B model checkpoint](https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth) is used in this project. But depending on the need others can be used.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380828d-a697-4926-afee-cc4588068d77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Libraries for SAM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3761394-fe0d-414e-aed5-7e6996a91d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299a33c-2d7d-4700-ba84-b3ebede7ebc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Rename Files\n",
    "\n",
    "Removing unnecessary parts from the image files to make it easier to read and access.  \n",
    "\n",
    "Running the next cell, will remove the **_DSC** , **leading zeros** and adds **Log_ID** in front of the name.  \n",
    "\n",
    "Supply the ID and file names manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0203f65-e29a-49f8-8c06-4bdd08e95c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign image_folder variable with 'path/to/folder' containing the image files\n",
    "image_folder = 'Reruns/extras'\n",
    "\n",
    "# Example dictionary mapping filenames to IDs (without _DSC and leading zeros)\n",
    "# Filenames will be converted to ID-filename format\n",
    "# Format: {'old_filename': 'ID'}\n",
    "file_id_mapping = {\n",
    "    '718': 'P241',\n",
    "    '719': 'P241',\n",
    "    '720': 'P241',\n",
    "    '721': 'P241',\n",
    "    '722': 'P241',\n",
    "    '723': 'P241',\n",
    "    '724': 'Pnocode',\n",
    "    '725': 'Pnocode',\n",
    "    '726': 'Pnocode',\n",
    "    '727': 'Pnocode',\n",
    "    '728': 'Pnocode',\n",
    "    '729': 'Pnocode',\n",
    "    '730': 'P250',\n",
    "    '731': 'P250',\n",
    "    '732': 'P250',\n",
    "    '733': 'P250',\n",
    "    '734': 'P250',\n",
    "    '735': 'P250',\n",
    "    '736': 'P245',\n",
    "    '737': 'P245',\n",
    "    '738': 'P245',\n",
    "    '739': 'P245',\n",
    "    '740': 'P245',\n",
    "    '741': 'P245',\n",
    "    '742': 'P249',\n",
    "    '743': 'P249',\n",
    "    '744': 'P249',\n",
    "    '745': 'P249',\n",
    "    '746': 'P249',\n",
    "    '747': 'P249',\n",
    "    '748': 'S250',\n",
    "    '749': 'S250',\n",
    "    '750': 'S250',\n",
    "    '751': 'S250',\n",
    "    '752': 'S250',\n",
    "    '753': 'S250',\n",
    "    '754': 'S243',\n",
    "    '755': 'S243',\n",
    "    '756': 'S243',\n",
    "    '757': 'S243',\n",
    "    '758': 'S243',\n",
    "    '759': 'S243',\n",
    "    '760': 'S242',\n",
    "    '761': 'S242',\n",
    "    '762': 'S242',\n",
    "    '763': 'S242',\n",
    "    '764': 'S242',\n",
    "    '765': 'S242',\n",
    "    '766': 'S249',\n",
    "    '767': 'S249',\n",
    "    '768': 'S249',\n",
    "    '769': 'S249',\n",
    "    '770': 'S249',\n",
    "    '771': 'S249',\n",
    "    '772': 'S246',\n",
    "    '773': 'S246',\n",
    "    '774': 'S246',\n",
    "    '775': 'S246',\n",
    "    '776': 'S246',\n",
    "    '777': 'S246'\n",
    "    \n",
    "}\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.JPG') or filename.endswith('.jpg'):  # Ensure it's a JPG file\n",
    "        # Extract the number part of the filename\n",
    "        number_part = filename.split('_DSC')[1].split('.')[0]  # Extract XXXX from _DSCXXXX.JPG\n",
    "        \n",
    "        # Remove leading zeros\n",
    "        number_part = str(int(number_part))\n",
    "        \n",
    "        # Get the corresponding ID from the dictionary\n",
    "        if number_part in file_id_mapping:\n",
    "            ID = file_id_mapping[number_part]\n",
    "            \n",
    "            # Construct the new filename with the ID prefix\n",
    "            new_filename = f\"{ID}-{number_part}.JPG\"\n",
    "            \n",
    "            # Construct the old and new file paths\n",
    "            old_file_path = os.path.join(image_folder, filename)\n",
    "            new_file_path = os.path.join(image_folder, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed {filename} to {new_filename}\")\n",
    "        else:\n",
    "            print(f\"No ID found for {number_part}, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21636e72-f1d7-4a19-b708-0034f9e8e622",
   "metadata": {},
   "source": [
    "## Prepare Model\n",
    "\n",
    "Taken from [Git Hub repo](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py)\n",
    "\n",
    "Arguments:\n",
    "* **model (Sam):** The SAM model to use for mask prediction.\n",
    "  \n",
    "* **points_per_side (int or None):** The number of points to be sampled along one side of the image. The total number of points is points_per_side**2. If None, 'point_grids' must provide explicit point sampling.\n",
    "  \n",
    "* **points_per_batch (int):** Sets the number of points run simultaneously by the model. Higher numbers may be faster but use more GPU memory.\n",
    "  \n",
    "* **pred_iou_thresh (float):** A filtering threshold in [0,1], using the model's predicted mask quality.\n",
    "  \n",
    "* **stability_score_thresh (float):** A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.\n",
    "  \n",
    "* **stability_score_offset (float):** The amount to shift the cutoff when calculated the stability score.\n",
    "  \n",
    "* **box_nms_thresh (float):** The box IoU cutoff used by non-maximal suppression to filter duplicate masks.\n",
    "  \n",
    "* **crop_n_layers (int):** If >0, mask prediction will be run again on crops of the image. Sets the number of layers to run, where each layer has 2**i_layer number of image crops.\n",
    "  \n",
    "* **crop_nms_thresh (float):** The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.  \n",
    "\n",
    "* **crop_overlap_ratio (float):** Sets the degree to which crops overlap. In the first crop layer, crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.  \n",
    "  \n",
    "* **crop_n_points_downscale_factor (int):** The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.  \n",
    "\n",
    "* **point_grids (list(np.ndarray) or None):** A list over explicit grids of points used for sampling, normalized to [0,1]. The nth grid in the list is used in the nth crop layer. Exclusive with points_per_side.  \n",
    "  \n",
    "* **min_mask_region_area (int):** If >0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than min_mask_region_area. Requires opencv.  \n",
    "\n",
    "* **output_mode (str):** The form masks are returned in. Can be 'binary_mask', 'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools. For large resolutions, 'binary_mask' may consume large amounts of memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6a452-b068-4b4c-8213-7bd5da59b25e",
   "metadata": {},
   "source": [
    "### Crop and Export Masks\n",
    "\n",
    "Largest mask is not consistent. Sometimes the background is the largest, or the object. So supervision is necessary. Top two largest areas are saved into dataframe and exported as CSV.  \n",
    "\n",
    "Dataframe has two columns:\n",
    "* ID\n",
    "* area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3365207-8ea5-4674-b942-d6c9ad3a212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# Add the path and import SAM modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Define parameters\n",
    "#sam_checkpoint = \"sam_vit_h_4b8939.pth\" # Huge model checkpoint\n",
    "#sam_checkpoint = \"sam_vit_l_0b3195.pth\" # Large model checkpoint\n",
    "sam_checkpoint = \"sam_vit_b_01ec64.pth\" # Base model checkpoint\n",
    "#model_type = \"vit_h\" # Huge model type\n",
    "#model_type = \"vit_l\" # Large model type\n",
    "model_type = \"vit_b\" # Base model type\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the SAM model\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Initialize the mask generator with adjusted parameters\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_batch=14, # Higher values can cause memory issue. Adjust according to VRAM \n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86, # 0.86\n",
    "    stability_score_thresh=0.95, # default 0.95\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=10000  # Focus on large objects?\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "!! !! DO NOT FORGET TO CHANGE root_dir and folder_names list to target folder !! !! \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the root directory containing all folders\n",
    "root_dir = 'ends_rerun/pine'\n",
    "\n",
    "# List of folder names to process\n",
    "folder_names = ['Site 1']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "!! !! DO NOT FORGET TO CHANGE OUTPUT DIRECTORY !! !! \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the output directory\n",
    "output_root_dir = 'ends_rerun\\cropped_images\\cropped\\pine'\n",
    "\n",
    "os.makedirs(output_root_dir, exist_ok=True)\n",
    "\n",
    "# Initialize a list to store mask data\n",
    "mask_data = []\n",
    "\n",
    "# Function to print messages with timestamps\n",
    "def print_with_timestamp(message):\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "# Process each folder\n",
    "for folder_name in folder_names:\n",
    "    input_dir = os.path.join(root_dir, folder_name)\n",
    "    output_dir = os.path.join(output_root_dir, folder_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    image_files = os.listdir(input_dir)\n",
    "\n",
    "    print_with_timestamp(f\"Total number of images to process in {folder_name}: {len(image_files)}\")\n",
    "\n",
    "    # Iterate through each image in the input directory\n",
    "    for idx, filename in enumerate(image_files):\n",
    "        print_with_timestamp(f\"Processing image {idx + 1}/{len(image_files)} in {folder_name}: {filename}\")\n",
    "\n",
    "        image_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Generate masks\n",
    "        masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "        # Sort masks by area in descending order\n",
    "        sorted_masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "\n",
    "        # Process the top 2 largest masks\n",
    "        for i in range(len(sorted_masks)):\n",
    "            mask_info = sorted_masks[i]\n",
    "\n",
    "            # Extract details of the mask\n",
    "            mask = mask_info['segmentation']\n",
    "            bbox = mask_info['bbox']\n",
    "            area = mask_info['area']\n",
    "\n",
    "            # Prepare filenames for output images\n",
    "            base_filename, ext = os.path.splitext(filename)\n",
    "            masked_output_path = os.path.join(output_dir, f'{base_filename}_masked_{i+1}{ext}')\n",
    "            #cropped_output_path = os.path.join(output_dir, f'{base_filename}_cropped_{i+1}{ext}') # Decresing clutter of exports\n",
    "            transparent_output_path = os.path.join(output_dir, f'{base_filename}_transparent_{i+1}.png')\n",
    "\n",
    "            # Append mask information to the dataframe\n",
    "            mask_data.append({\n",
    "                'ID': f'{base_filename}_cropped_{i+1}{ext}',\n",
    "                'area': area,\n",
    "                'top_left_x': bbox[0],\n",
    "                'top_left_y': bbox[1]\n",
    "            })\n",
    "\n",
    "            # Apply mask to the image\n",
    "            masked_image = image_rgb.copy()\n",
    "            masked_image[mask == 0] = 0  # Black out the areas not in the mask\n",
    "\n",
    "            # Save the masked output image (image with mask applied)\n",
    "            masked_image_bgr = cv2.cvtColor(masked_image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(masked_output_path, masked_image_bgr)\n",
    "\n",
    "            # Ensure bounding box coordinates are integers\n",
    "            x, y, w, h = map(int, bbox)\n",
    "\n",
    "            # Crop the image using the mask's bounding box\n",
    "            cropped_image = masked_image[y:y+h, x:x+w]\n",
    "            cropped_image_bgr = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)\n",
    "            #cv2.imwrite(cropped_output_path, cropped_image_bgr)\n",
    "\n",
    "            # Create a transparent image using PIL\n",
    "            image_pil = Image.fromarray(image_rgb)\n",
    "            mask_pil = Image.fromarray((mask * 255).astype(np.uint8)).convert(\"L\")\n",
    "\n",
    "            # Create an RGBA image with transparency\n",
    "            transparent_image_pil = Image.new(\"RGBA\", image_pil.size)\n",
    "            transparent_image_pil.paste(image_pil, (0, 0), mask_pil)\n",
    "\n",
    "            # Crop the transparent image\n",
    "            transparent_cropped = transparent_image_pil.crop((x, y, x + w, y + h))\n",
    "\n",
    "            # Save the transparent image\n",
    "            transparent_cropped.save(transparent_output_path)\n",
    "\n",
    "        print_with_timestamp(f\"Completed processing image: {filename}\")\n",
    "\n",
    "        # Clear GPU memory and run garbage collection after processing each image\n",
    "        del masks\n",
    "        del sorted_masks\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Convert mask data to a pandas dataframe\n",
    "df_new = pd.DataFrame(mask_data)\n",
    "\n",
    "# Define the path to the CSV file in the current working directory\n",
    "csv_output_path = os.path.join(os.getcwd(), 'mask_areas.csv')\n",
    "\n",
    "# Check if the CSV file exists and append new data\n",
    "if os.path.exists(csv_output_path):\n",
    "    df_existing = pd.read_csv(csv_output_path)\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    df_combined.to_csv(csv_output_path, index=False)\n",
    "else:\n",
    "    df_new.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# Clear GPU memory at the end of run\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print_with_timestamp(\"All images have been processed and results have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb0551-f691-4b31-b14f-9f949131c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "del masks\n",
    "del sorted_masks\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8ff71-0ddb-4168-a550-9fe36f3b817f",
   "metadata": {},
   "source": [
    "# Data Clean Up and Data Set Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7e7db-5b37-40cb-8627-e7fd81dea4be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Drop Lines Containing Zero and Clean \"ID\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e766e6-5cb7-41b5-b098-390c719335a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file = \"mask_areas.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "#  Count the initial number of rows\n",
    "initial_row_count = len(df)\n",
    "print(f\"Initial number of rows: {initial_row_count}\")\n",
    "\n",
    "# Find rows that contain zero in any column\n",
    "rows_with_zero = df[(df == 0).any(axis=1)]\n",
    "print(f\"Total count of rows contain zero {len(rows_with_zero)}\")\n",
    "\n",
    "# Print the content of rows to be removed\n",
    "#print(\"Rows to be removed (contain zero):\")\n",
    "#for index, row in rows_with_zero.iterrows():\n",
    "#    print(f\"Row{index}:\")\n",
    "#    print(row)\n",
    "\n",
    "# Drop rows that contain zero\n",
    "df = df[(df != 0).all(axis=1)]\n",
    "\n",
    "# Count the final number of rows\n",
    "final_row_count = len(df)\n",
    "print(f\"Final number of rows: {final_row_count}\")\n",
    "\n",
    "# Save the updated CSV (without removed rows)\n",
    "df.to_csv(\"mask_areas_no_zero.csv\", index=False)  # Update this with your output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138bebaf-8f0e-4e7b-94ab-2de86446daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "original_csv_path = 'mask_areas_no_zero.csv'\n",
    "df = pd.read_csv(original_csv_path)\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original one\n",
    "new_df = df.copy()\n",
    "\n",
    "# Modify the 'ID' column, replacing 'cropped' with 'transparent' and 'JPG' with 'png'\n",
    "new_df['ID'] = new_df['ID'].str.replace('JPG', 'png')\n",
    "\n",
    "# Define the path or name to save the new CSV file\n",
    "new_csv_path = 'mask_areasIDmod.csv'\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "new_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(\"New dataset has been created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b23203-d0d1-4664-84ca-45f8238ea776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pine Data Set Build\n",
    "\n",
    "There are duplicates and wrongly named files due to running images with different model sizes. In some cases Background and log mask named similar. This led to inaccurate cleaning of the data.\n",
    "\n",
    "First approach to clean the data as follows:\n",
    "\n",
    "1) Iterate through each image folder\n",
    "2) Cross check with the mask_area.csv\n",
    "3) If image name and **ID** in mask_area.csv matches save the row to new dataframe\n",
    "4) Export dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef86948-e1ea-4f08-b243-e348d400e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ID modified data set into dataframe\n",
    "csv_path = 'mask_areasIDmod.csv'\n",
    "IDmod_df = pd.read_csv(csv_path)\n",
    "\n",
    "# New dataframe to store matched rows\n",
    "matched_df = pd.DataFrame(columns=IDmod_df.columns)\n",
    "\n",
    "# Define path to folders to be iterated\n",
    "main_folder = 'pine_sites'\n",
    "\n",
    "# Iterate through each subfolder in the main folder\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "    # Path to images\n",
    "    image_folder_path = os.path.join(subfolder_path, 'cropped')\n",
    "\n",
    "    # Checl if the folder exists\n",
    "    if os.path.isdir(image_folder_path):\n",
    "        # Iterate through each image in 'cropped'\n",
    "        for image_file in os.listdir(image_folder_path):\n",
    "            # Compare full file name with 'ID' column of mask_areas_IDmod.csv\n",
    "            matching_rows = IDmod_df[IDmod_df['ID'].str.contains(image_file, case=False, na=False)]\n",
    "\n",
    "            if not matching_rows.empty:\n",
    "                matched_df = pd.concat([matched_df, matching_rows])\n",
    "print(len(matched_df))\n",
    "# Export the matched dataframe to new CSV file\n",
    "output_csv_path = 'mask_area_modified_pine.csv'\n",
    "matched_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Matched data has been saved to {output_csv_path}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbaefad-34be-420e-9e57-fdb7ef4bc8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the ID column. Remove unnecessary indicators that came from SAM output.\n",
    "\n",
    "# Load CSV from previous step to clean ID column's rows.\n",
    "csv_path = 'mask_area_modified_pine.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Remove characters after '_' in the ID column\n",
    "df['ID'] = df['ID'].str.split('_').str[0]\n",
    "\n",
    "# Save the modified dataframe to new CSV\n",
    "output_csv_path = 'mask_area_modified_pine_ID_cleaned.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Modified CSV has been saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986172a-4fa7-4907-9e0e-093369254002",
   "metadata": {},
   "source": [
    "There seems to be some duplicate names and coordinates with zero values in the data set. Only the rows with no zero values are needed. Duplicates need further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfa307-6b23-4866-b939-76ac69a52660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('mask_area_modified_pine_ID_cleaned.csv')\n",
    "\n",
    "# Find rows with zero values in the entire dataframe\n",
    "zero_value_rows = df[(df == 0).any(axis=1)]\n",
    "\n",
    "# Display the rows with zero values\n",
    "print(zero_value_rows)\n",
    "\n",
    "# Find duplicates in the 'Log_ID' column\n",
    "#duplicates = df[df.duplicated(subset='ID', keep=False)]\n",
    "\n",
    "# Display the duplicate rows\n",
    "#print(\"\\nDuplicate Rows Based on 'ID' Column:\")\n",
    "#print(duplicates)\n",
    "\n",
    "# Optionally, save the duplicates to a new CSV file\n",
    "#duplicates.to_csv('duplicate_area_pine.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6551f3-5111-4185-8135-532d5eb66969",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pine Data Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898f0b3-0ad2-45ae-9397-bde0c600e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'pine_merged_dataset.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_pine = pd.read_csv(csv_file_path)\n",
    "print(len(df_pine))\n",
    "\n",
    "# Find and remove exact duplicate rows, keeping the first occurrence\n",
    "df_unique_pine = df_pine.drop_duplicates(keep='first')\n",
    "print(len(df_unique_pine))\n",
    "#Check for duplicates (ID column)\n",
    "id_column = 'ID'\n",
    "\n",
    "# Find duplicate IDs\n",
    "duplicate_ids = df_unique_pine[df_unique_pine[id_column].duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicates\n",
    "print(f\"Total number of duplicate rows in the '{id_column}' column: {len(duplicate_ids)}\")\n",
    "#print(duplicate_ids)\n",
    "print(len(df_unique_pine))\n",
    "print(len(duplicate_ids))\n",
    "# Iterate through the duplicate IDs\n",
    "\n",
    "#for index, row in duplicate_ids.iterrows():\n",
    "#    print(f\"\\nShowing duplicate row {index}:\")\n",
    "#    print(row)\n",
    "\n",
    "with open('pine_duplicates_2.txt', 'w') as f:\n",
    "    for index, row in duplicate_ids.iterrows():\n",
    "        f.write(f\"\\nShowing duplicate row {index}:\\n\")\n",
    "        f.write(row.to_string())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cee11-51b9-4a45-b442-cfb3288e650b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Spruce Data Set Build\n",
    "\n",
    "Prepare file names for next step. Change transparent > cropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e275ec8-6dbe-45ce-8ec3-c1a30c2b88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main directory containing all subfolders\n",
    "main_folder = 'spruce_sites'\n",
    "\n",
    "# Function to rename files in each 'cropped' folder\n",
    "def rename_files_in_folder(folder_path):\n",
    "    # The path to the 'cropped' folder inside the current subfolder\n",
    "    cropped_folder_path = os.path.join(folder_path, 'cropped')\n",
    "    \n",
    "    if not os.path.exists(cropped_folder_path):\n",
    "        print(f\"No 'cropped' folder found in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    # Iterate through each file in the 'cropped' folder\n",
    "    for filename in os.listdir(cropped_folder_path):\n",
    "        # Check if 'transparent' is in the filename\n",
    "        if 'transparent' in filename:\n",
    "            # Create the new filename by replacing 'transparent' with 'cropped'\n",
    "            new_filename = filename.replace('transparent', 'cropped')\n",
    "            \n",
    "            # Get the full paths for the old and new filenames\n",
    "            old_file_path = os.path.join(cropped_folder_path, filename)\n",
    "            new_file_path = os.path.join(cropped_folder_path, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed: {filename} to {new_filename}\")\n",
    "\n",
    "# Iterate through all subfolders in the main folder\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    # Check if it's a directory (subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        rename_files_in_folder(subfolder_path)\n",
    "\n",
    "print(\"File renaming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d9558-836e-4547-9dd6-87374ab1db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mask_areasIDmod.csv' file is created earlier under Pine Data Set Build section of this notebook.\n",
    "\n",
    "# Load the ID modified data set into dataframe\n",
    "csv_path = 'mask_areasIDmod.csv'\n",
    "IDmod_df = pd.read_csv(csv_path)\n",
    "\n",
    "# New dataframe to store matched rows\n",
    "matched_df = pd.DataFrame(columns=IDmod_df.columns)\n",
    "\n",
    "# Define path to folders to be iterated - spruce_sites\n",
    "main_folder = 'spruce_sites'\n",
    "\n",
    "# Iterate through each subfolder in the main folder\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "    # Path to images\n",
    "    image_folder_path = os.path.join(subfolder_path, 'cropped')\n",
    "\n",
    "    # Checl if the folder exists\n",
    "    if os.path.isdir(image_folder_path):\n",
    "        # Iterate through each image in 'cropped'\n",
    "        for image_file in os.listdir(image_folder_path):\n",
    "            # Compare full file name with 'ID' column of mask_areas_IDmod.csv\n",
    "            matching_rows = IDmod_df[IDmod_df['ID'].str.contains(image_file, case=False, na=False)]\n",
    "\n",
    "            if not matching_rows.empty:\n",
    "                matched_df = pd.concat([matched_df, matching_rows])\n",
    "print(len(matched_df))\n",
    "# Export the matched dataframe to new CSV file\n",
    "output_csv_path = 'mask_area_modified_spruce.csv'\n",
    "matched_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Matched data has been saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd98989-3f60-4cf2-9065-63d10860978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the ID column. Remove unnecessary indicators that came from SAM output.\n",
    "\n",
    "# Load CSV from previous step to clean ID column's rows.\n",
    "csv_path = 'mask_area_modified_spruce.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Remove characters after '_' in the ID column\n",
    "df['ID'] = df['ID'].str.split('_').str[0]\n",
    "\n",
    "# Save the modified dataframe to new CSV\n",
    "output_csv_path = 'mask_area_modified_spruce_ID_cleaned.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Modified CSV has been saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4cd9b-dffe-4243-bcf4-38e1043d7de2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Spruce Data Duplicate Rows\n",
    "\n",
    "Export duplicate row information to TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837077c-9f27-4357-80aa-f397ab2d9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'raw_data/mask_area_modified_spruce_ID_cleaned.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_spruce = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Find and remove exact duplicate rows, keeping the first occurrence\n",
    "df_unique_spruce = df_spruce.drop_duplicates(keep='first')\n",
    "\n",
    "#Check for duplicates (ID column)\n",
    "id_column = 'ID'\n",
    "\n",
    "# Find duplicate IDs\n",
    "duplicate_ids = df_unique_spruce[df_unique_spruce[id_column].duplicated(keep=False)]\n",
    "\n",
    "# Print the duplicates\n",
    "print(f\"Total number of duplicate rows in the '{id_column}' column: {len(duplicate_ids)}\")\n",
    "#print(duplicate_ids)\n",
    "\n",
    "print(len(df_spruce))\n",
    "print(len(df_unique_spruce))\n",
    "print(len(duplicate_ids))\n",
    "\n",
    "# Iterate through the duplicate IDs\n",
    "#for index, row in duplicate_ids.iterrows():\n",
    "#    print(f\"\\nShowing duplicate row {index}:\")\n",
    "#    print(row)\n",
    "\n",
    "with open('spruce_duplicates.txt', 'w') as f:\n",
    "    for index, row in duplicate_ids.iterrows():\n",
    "        f.write(f\"\\nShowing duplicate row {index}:\\n\")\n",
    "        f.write(row.to_string())\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3aec1f-7f76-4051-929d-4ab2042a0557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove Duplicate Rows - Pines\n",
    "\n",
    "Using duplicate data set, found the rows that contain duplicate or wrong data. Took note of those rows and they will be removed in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd076e3b-03be-4440-8728-f75ddddaf4d1",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "#df_pine = pd.read_csv(\"raw_data/mask_area_modified_pine_ID_cleaned.csv\")\n",
    "df_pine = pd.read_csv(\"cleaned_data/pine_merged_dataset.csv\")\n",
    "\n",
    "# Remove exact duplicates, keeping the first occurance\n",
    "df_unique_pine = df_pine.drop_duplicates(keep='first')\n",
    "print(len(df_unique_pine))\n",
    "\n",
    "# Load row indices to be removed from a TXT file\n",
    "with open('pine_wrong_rows.txt', 'r') as file:\n",
    "    rows_to_remove_str = file.read()\n",
    "\n",
    "# Convert the string of numbers into a list of integers\n",
    "rows_to_remove = list(map(int, rows_to_remove_str.split(', ')))\n",
    "print(len(rows_to_remove))\n",
    "\n",
    "# Check if row indices are valid before removing\n",
    "if max(rows_to_remove) < len(df_unique_pine):\n",
    "    # Print the content of rows to be removed\n",
    "    rows_content_to_remove = df_unique_pine.iloc[rows_to_remove]\n",
    "    print(\"Rows to be removed:\")\n",
    "    print(rows_content_to_remove)\n",
    "\n",
    "    # Print the content of rows to be removed one by one\n",
    "    print(\"Rows to be removed (duplicate or wrong):\")\n",
    "    for index, row in rows_content_to_remove.iterrows():\n",
    "        print(f\"Row {index}:\")\n",
    "        print(row)\n",
    "        print('-' * 50)  # Separator for readability\n",
    "\n",
    "# Remove the rows using row index numbers\n",
    "df_nodupe_pine = df_unique_pine.drop(rows_to_remove)\n",
    "\n",
    "# Count the final number of rows\n",
    "count_rows_to_remove = len(rows_to_remove)\n",
    "row_count_before = len(df_unique_pine)\n",
    "final_row_count = len(df_nodupe_pine)\n",
    "print(f\"Row count before: {row_count_before} \\nFinal number of rows: {final_row_count} \\nRows removed {count_rows_to_remove}\")\n",
    "\n",
    "# Save the updated DataFrame to CSV (optional)\n",
    "df_nodupe_pine.to_csv(\"raw_data/pine_data_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb175d2c-a97e-4e5c-8e63-a3504b8f95b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Correction to Data Typos\n",
    "\n",
    "Small p replaced with Capital P for p148\n",
    "\n",
    "P230 is recorded as P23. Currently there is only one P230 which is P23-783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea92e9-f528-4974-b3ed-7ba62f4bbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "data = pd.read_csv(\"raw_data/mask_area_modified_pine_ID_cleaned.csv\")\n",
    "df_pine_typo = pd.DataFrame(data)\n",
    "\n",
    "# 1. Modify specific IDs\n",
    "# Create a dictionary with old and new IDs\n",
    "id_replacements = {\n",
    "    'p148-401': 'P148-401',\n",
    "    'p237-365': 'P237-365'\n",
    "}\n",
    "\n",
    "# Replace IDs based on the dictionary\n",
    "df_pine_typo['ID'] = df_pine_typo['ID'].replace(id_replacements)\n",
    "\n",
    "# 2. Correct lowercase 'p' with uppercase 'P' in all columns\n",
    "df_pine_typo = df_pine_typo.applymap(lambda x: x.replace('p', 'P') if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated DataFrame to CSV (optional)\n",
    "df_pine_typo.to_csv(\"cleaned_data/pine_data_cleaned.csv\", index=False)  # Uncomment when exporting the data and specify the output file name and folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d487e3-b56b-4997-8213-a2d239f9145a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Report of ID Counts for Pines\n",
    "\n",
    "**Fixing typos in the naming by hand on the data file** \n",
    "* p237 -> P237\n",
    "* p148 -> P148\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d0777-023a-4e7d-a15a-74c312a4ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "data = pd.read_csv(\"cleaned_data/pine_data_cleaned.csv\")\n",
    "df_corrected = pd.DataFrame(data)\n",
    "\n",
    "# Extract the repeating part of the ID (the part before '-')\n",
    "df_corrected['ID_Part'] = df_corrected['ID'].str.split('-').str[0]\n",
    "\n",
    "# Count how many times each ID_Part occurs\n",
    "id_counts = df_corrected['ID_Part'].value_counts()\n",
    "\n",
    "# Identify IDs that don't appear 6 times\n",
    "ids_missing = id_counts[id_counts != 6]\n",
    "\n",
    "#Print the IDs and their occurrence counts (which are not 6)\n",
    "#print(\"IDs missing some entries (not appearing 6 times):\")\n",
    "#print(ids_missing)\n",
    "\n",
    "# Print the missing IDs and their occurrences specifically\n",
    "for id_part, count in ids_missing.items():\n",
    "    print(f\"ID: {id_part}, Present {count} times (Missing {6 - count} times)\")\n",
    "\n",
    "# Open a text file to write the output\n",
    "with open('id_counts/spruce_idcounts_report.txt', 'w') as file:\n",
    "    for id_part, count in ids_missing.items():\n",
    "        file.write(f\"ID: {id_part}, Present {count} times (Missing {6 - count} times)\\n\")\n",
    "\n",
    "print(\"Output written to ids_report.txt\")\n",
    "\n",
    "# Full row data of the missing IDs,  can filter them out\n",
    "missing_rows = df_corrected[df_corrected['ID_Part'].isin(ids_missing.index)]\n",
    "print(\"Rows with IDs missing:\")\n",
    "# print(missing_rows)\n",
    "\n",
    "# Define the specific ID_Part you're looking for\n",
    "specific_id_part = \"P23\"  # Replace with the specific ID part you want to print\n",
    "\n",
    "# Filter the rows where ID_Part matches the specific ID\n",
    "matching_rows = df_corrected[df_corrected['ID_Part'] == specific_id_part]\n",
    "\n",
    "# Print the rows with the specific ID\n",
    "print(f\"Rows with ID part '{specific_id_part}':\")\n",
    "print(matching_rows)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a05d72-168d-46e5-aa84-be9e61252526",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove Duplicate Rows - Spruce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e62509-0b90-49b2-942f-af143142ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "df_spruce = pd.read_csv(\"raw_data/mask_area_modified_spruce_ID_cleaned.csv\")\n",
    "print(len(df_spruce))\n",
    "\n",
    "# Remove exact duplicates, keeping the first occurance\n",
    "df_unique_spruce = df_spruce.drop_duplicates(keep='first')\n",
    "print(len(df_unique_spruce))\n",
    "print(df_unique_spruce)\n",
    "\n",
    "# Load row indices to be removed from a TXT file\n",
    "with open('spruce_wrong_rows.txt', 'r') as file:\n",
    "    rows_to_remove_str = file.read()\n",
    "\n",
    "# Convert the string of numbers into a list of integers\n",
    "rows_to_remove = list(map(int, rows_to_remove_str.split(', ')))\n",
    "print(len(rows_to_remove))\n",
    "print(rows_to_remove)\n",
    "\n",
    "# Check if row indices are valid before removing\n",
    "if max(rows_to_remove) < len(df_unique_spruce):\n",
    "    # Print the content of rows to be removed\n",
    "    rows_content_to_remove = df_unique_spruce.iloc[rows_to_remove]\n",
    "    print(\"Rows to be removed:\")\n",
    "    print(rows_content_to_remove)\n",
    "\n",
    "    # Print the content of rows to be removed one by one\n",
    "    print(\"Rows to be removed (duplicate or wrong):\")\n",
    "    for index, row in rows_content_to_remove.iterrows():\n",
    "        print(f\"Row {index}:\")\n",
    "        print(row)\n",
    "        print('-' * 50)  # Separator for readability\n",
    "\n",
    "# Remove the rows using row index numbers\n",
    "df_nodupe_spruce = df_unique_spruce.drop(rows_to_remove)\n",
    "\n",
    "# Count the final number of rows\n",
    "count_rows_to_remove = len(rows_to_remove)\n",
    "row_count_before = len(df_unique_spruce)\n",
    "final_row_count = len(df_nodupe_spruce)\n",
    "print(f\"Row count before: {row_count_before} \\nFinal number of rows: {final_row_count} \\nRows removed {count_rows_to_remove}\")\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df_nodupe_spruce.to_csv(\"cleaned_data/mask_area_modified_spruce_nodupe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040f9ef-5ade-4136-ad1e-49d50a14fbdc",
   "metadata": {},
   "source": [
    "## Join Missing Spruce Data with the Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed680568-22f6-4b63-ba9d-221c06215e22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Correction to Data Typos\n",
    "\n",
    "S186-1082, 1083, 1084, 1085, 1086, 1087 should be **S168**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995e3d2-3c53-4d6f-9011-d2b1b8bfd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "data = pd.read_csv(\"cleaned_data/mask_area_modified_spruce_nodupe.csv\")\n",
    "df_spruce_typo = pd.DataFrame(data)\n",
    "\n",
    "# 1. Modify specific IDs\n",
    "# Create a dictionary with old and new IDs\n",
    "id_replacements = {\n",
    "    'S186-1082': 'S168-1082',\n",
    "    'S186-1083': 'S168-1083',\n",
    "    'S186-1084': 'S168-1084',\n",
    "    'S186-1085': 'S168-1085',\n",
    "    'S186-1086': 'S168-1086',\n",
    "    'S186-1087': 'S168-1087',    \n",
    "}\n",
    "\n",
    "# Replace IDs based on the dictionary\n",
    "df_spruce_typo['ID'] = df_spruce_typo['ID'].replace(id_replacements)\n",
    "\n",
    "# Save the updated DataFrame to CSV (optional)\n",
    "df_spruce_typo.to_csv(\"cleaned_data/spruce_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb378f3b-2b39-4f0a-ba54-944b110f376d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Merge the Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c5db5-9549-4a87-8e83-2a11edec3003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('spruce_missing_data2.csv')\n",
    "\n",
    "# Remove everything after \"_\" in the \"ID\" column\n",
    "df['ID'] = df['ID'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# Save the cleaned CSV file\n",
    "df.to_csv('spruce_missing_cleaned_file.csv', index=False)\n",
    "\n",
    "print(\"IDs cleaned and saved\")\n",
    "\n",
    "# Load your cleaned CSV and main dataset\n",
    "df_cleaned = df\n",
    "df_main = pd.read_csv('cleaned_data/spruce_data_cleaned.csv')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_merged = pd.concat([df_cleaned, df_main], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset\n",
    "df_merged.to_csv('spruce_merged_dataset.csv', index=False)\n",
    "\n",
    "print(\"Merging complete. Saved as 'spruce_merged_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0eff7-92c1-4426-92ed-abefd2ca73df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Image Normalization per Log - Best Option\n",
    "\n",
    "Uses blanket of the image to normalize (1 blanket per 1 log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3191f-4861-4e16-928a-398e3a12ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def normalize_images_in_subfolders(main_folder, subfolder_names):\n",
    "    \"\"\"\n",
    "    Normalize images across multiple subfolders. Each subfolder should contain\n",
    "    'background' and 'cropped' folders, and the normalized images will be saved in a\n",
    "    'normalized' folder within the same subfolder.\n",
    "\n",
    "    Args:\n",
    "        main_folder (str): Path to the main folder containing subfolders.\n",
    "        subfolder_names (list): List of subfolder names to process.\n",
    "    \"\"\"\n",
    "    for subfolder_name in subfolder_names:\n",
    "        subfolder_path = os.path.join(main_folder, subfolder_name)\n",
    "        background_folder = os.path.join(subfolder_path, \"background\")\n",
    "        cropped_folder = os.path.join(subfolder_path, \"cropped\")\n",
    "        normalized_folder = os.path.join(subfolder_path, \"normalized\")\n",
    "        \n",
    "        # Create the 'normalized' folder if it doesn't exist\n",
    "        os.makedirs(normalized_folder, exist_ok=True)\n",
    "\n",
    "        # Check if 'background' and 'cropped' folders exist\n",
    "        if not os.path.exists(background_folder) or not os.path.exists(cropped_folder):\n",
    "            print(f\"Skipping {subfolder_name}: Missing 'background' or 'cropped' folder.\")\n",
    "            continue\n",
    "\n",
    "        # Get sorted lists of images\n",
    "        background_images = {f: os.path.join(background_folder, f) for f in os.listdir(background_folder) if f.lower().endswith('.png')}\n",
    "        cropped_images = {f: os.path.join(cropped_folder, f) for f in os.listdir(cropped_folder) if f.lower().endswith('.png')}\n",
    "\n",
    "        # Find matching image names\n",
    "        matching_names = set(background_images.keys()) & set(cropped_images.keys())\n",
    "        if not matching_names:\n",
    "            print(f\"Skipping {subfolder_name}: No matching image names between 'background' and 'cropped' folders.\")\n",
    "            continue\n",
    "\n",
    "        for image_name in sorted(matching_names):\n",
    "            background_path = background_images[image_name]\n",
    "            cropped_path = cropped_images[image_name]\n",
    "\n",
    "            # Load images\n",
    "            RGB_background = cv2.imread(background_path, cv2.IMREAD_UNCHANGED)\n",
    "            RGB_cropped = cv2.imread(cropped_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            if RGB_background is None:\n",
    "                print(f\"Error: Cannot read the background image at {background_path}\")\n",
    "                continue\n",
    "            if RGB_cropped is None:\n",
    "                print(f\"Error: Cannot read the cropped image at {cropped_path}\")\n",
    "                continue\n",
    "\n",
    "            # Separate RGB and alpha channels\n",
    "            RGB_background, alpha_background = RGB_background[..., :3], RGB_background[..., 3]\n",
    "            RGB_cropped, alpha_cropped = RGB_cropped[..., :3], RGB_cropped[..., 3]\n",
    "\n",
    "            # Compute HSV means for normalization\n",
    "            h_mean, s_mean, v_mean = compute_hsv_means(RGB_background, alpha_background)\n",
    "\n",
    "            # Normalize and save the image\n",
    "            export_normalized_image(RGB_cropped, alpha_cropped, h_mean, s_mean, v_mean, normalized_folder, image_name)\n",
    "\n",
    "        print(f\"Normalization completed for {subfolder_name}. Images saved in '{normalized_folder}'.\")\n",
    "\n",
    "def compute_hsv_means(RGB_image, alpha_channel):\n",
    "    \"\"\"\n",
    "    Compute the mean hue, saturation, and value for the given image.\n",
    "    \"\"\"\n",
    "    HSV_image = cv2.cvtColor(RGB_image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "\n",
    "    # Normalize ranges to [0, 1]\n",
    "    HSV_image[..., 0] /= 179  # Hue to [0, 1]\n",
    "    HSV_image[..., 1] /= 255  # Saturation to [0, 1]\n",
    "    HSV_image[..., 2] /= 255  # Value to [0, 1]\n",
    "\n",
    "    # Apply mask\n",
    "    mask = alpha_channel > 0\n",
    "    hue_vals = HSV_image[..., 0][mask]\n",
    "    sat_vals = HSV_image[..., 1][mask]\n",
    "    val_vals = HSV_image[..., 2][mask]\n",
    "\n",
    "    # Calculate mean, ignoring NaNs\n",
    "    h_mean = np.nanmean(hue_vals)\n",
    "    s_mean = np.nanmean(sat_vals)\n",
    "    v_mean = np.nanmean(val_vals)\n",
    "\n",
    "    return h_mean, s_mean, v_mean\n",
    "\n",
    "def export_normalized_image(RGB_image, alpha_channel, h_mean, s_mean, v_mean, output_folder, filename):\n",
    "    \"\"\"\n",
    "    Normalize an RGB image by adjusting its saturation and value and save it.\n",
    "    \"\"\"\n",
    "    s_target = 0.05\n",
    "    v_target = 0.9\n",
    "\n",
    "    s_shift = s_target - s_mean\n",
    "    v_shift = v_target - v_mean\n",
    "\n",
    "    # Convert RGB to HSV\n",
    "    HSV_image = cv2.cvtColor(RGB_image, cv2.COLOR_BGR2HSV).astype(np.float32)\n",
    "\n",
    "    # Adjust saturation and value\n",
    "    HSV_image[..., 1] = np.clip(HSV_image[..., 1] + s_shift * 255, 0, 255)  # Adjust Saturation\n",
    "    HSV_image[..., 2] = np.clip(HSV_image[..., 2] + v_shift * 255, 0, 255)  # Adjust Value\n",
    "\n",
    "    # Convert back to BGR using the modified HSV\n",
    "    RGB_modified = cv2.cvtColor(HSV_image.astype(np.uint8), cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Create an output image with 4 channels (RGBA)\n",
    "    output_image = np.zeros((*RGB_image.shape[:2], 4), dtype=np.uint8)\n",
    "\n",
    "    # Assign RGB values to the output image\n",
    "    output_image[..., :3] = RGB_modified  # RGB channels\n",
    "\n",
    "    # Assign the alpha channel\n",
    "    output_image[..., 3] = alpha_channel  # Alpha channel from the original image\n",
    "\n",
    "    # Ensure transparent areas are set to (0, 0, 0, 0)\n",
    "    output_image[alpha_channel == 0] = [0, 0, 0, 0]  # Set transparent areas\n",
    "\n",
    "    # Save the output image\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    cv2.imwrite(output_path, output_image)\n",
    "\n",
    "# Folders\n",
    "main_folder = 'D:\\internship_images/pine_sites'\n",
    "subfolder_names = ['Site 21']\n",
    "\n",
    "normalize_images_in_subfolders(main_folder, subfolder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379be7d0-22ec-461d-b3aa-9cc86abc19f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lichen Area - Specific Images Final Algorithm V3\n",
    "\n",
    "Final Iteration. Designed to works on provided individual logs for increased accuracy but time consuming. Because HSV data should be collected using GIMP for each individual log from 4 different images.\n",
    "\n",
    "* Uses provided file names. Because applying same HSV ranges to all images did not yield good results\n",
    "* Collect pixel data using GIMP. There is a built in GIMP to cv2 conversion function for HSV ranges.\n",
    "* \"no_growth_images\" list used for assigning zero to lichen related features for log that has no lichen growth\n",
    "* Finds pixels in given HSV range and counts\n",
    "* Overlap pixels taken into account using bitwise comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc1e00-1b2a-448e-ade5-6a9278657256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Convert GIMP HSV to OpenCV HSV\n",
    "def convert_hsv_gimp_to_opencv(hsv_gimp):\n",
    "    h, s, v = hsv_gimp\n",
    "    return np.array([h / 2, s * 2.55, v * 2.55], dtype=np.uint8)\n",
    "\n",
    "# Process a single image and return the calculated areas\n",
    "def process_single_image(image_path, opencv_green_bounds, opencv_white_bounds, opencv_ignored_bounds, output_folder):\n",
    "    file_name = os.path.basename(image_path)\n",
    "    file_name_no_ext = os.path.splitext(file_name)[0]  # Remove extension\n",
    "    file_id = file_name.split('-')[0]  # Extract part before the first hyphen\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to load {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create masks for green, white, and ignored ranges\n",
    "    green_mask = cv2.inRange(hsv_image, *opencv_green_bounds)\n",
    "    white_mask = cv2.inRange(hsv_image, *opencv_white_bounds)\n",
    "\n",
    "    # Ignored mask\n",
    "    ignored_mask = None\n",
    "    if opencv_ignored_bounds:\n",
    "        ignored_mask = np.zeros_like(green_mask)\n",
    "        for bounds in opencv_ignored_bounds:\n",
    "            ignored_mask = cv2.bitwise_or(ignored_mask, cv2.inRange(hsv_image, *bounds))\n",
    "    \n",
    "    # Overlapping mask: Pixels detected in both green and white\n",
    "    overlap_mask = cv2.bitwise_and(green_mask, white_mask)\n",
    "    \n",
    "    # Exclusive areas: Remove overlap from both masks\n",
    "    green_exclusive = cv2.bitwise_and(green_mask, cv2.bitwise_not(overlap_mask))\n",
    "    white_exclusive = cv2.bitwise_and(white_mask, cv2.bitwise_not(overlap_mask))\n",
    "    \n",
    "    # Remove ignored pixels\n",
    "    if ignored_mask is not None:\n",
    "        green_exclusive = cv2.bitwise_and(green_exclusive, cv2.bitwise_not(ignored_mask))\n",
    "        white_exclusive = cv2.bitwise_and(white_exclusive, cv2.bitwise_not(ignored_mask))\n",
    "        overlap_mask = cv2.bitwise_and(overlap_mask, cv2.bitwise_not(ignored_mask))\n",
    "    \n",
    "    # Count pixels\n",
    "    green_exclusive_area = np.sum(green_exclusive == 255)\n",
    "    white_exclusive_area = np.sum(white_exclusive == 255)\n",
    "    overlap_area = np.sum(overlap_mask == 255)\n",
    "    \n",
    "    # Total area calculation\n",
    "    total_area = green_exclusive_area + white_exclusive_area + overlap_area\n",
    "    total_green_area = np.sum(green_mask == 255)\n",
    "    total_white_area = np.sum(white_mask == 255)\n",
    "\n",
    "    # Save segmented images\n",
    "    save_segmented_images(image, green_exclusive, white_exclusive, overlap_mask, file_name_no_ext, output_folder)\n",
    "\n",
    "    return (file_id, file_name_no_ext, total_green_area, total_white_area, green_exclusive_area, white_exclusive_area, overlap_area, total_area)\n",
    "\n",
    "# Save the segmented images\n",
    "def save_segmented_images(original_image, green_exclusive, white_exclusive, overlap_mask, file_name_no_ext, output_folder):\n",
    "    colorized_green = cv2.merge([np.zeros_like(green_exclusive), green_exclusive, np.zeros_like(green_exclusive)])\n",
    "    colorized_white = cv2.merge([white_exclusive, white_exclusive, white_exclusive])\n",
    "    combined_segmented = cv2.addWeighted(colorized_green, 1, colorized_white, 1, 0)\n",
    "    combined_segmented[overlap_mask > 0] = [0, 0, 255]  # Red for overlap\n",
    "    cv2.imwrite(os.path.join(output_folder, f'{file_name_no_ext}.jpg'), combined_segmented)\n",
    "\n",
    "# Process images from lists\n",
    "def process_images(target_folder, growth_images, no_growth_images, output_folder, csv_output_path, gimp_green_bounds, gimp_white_bounds, gimp_ignored_bounds):\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    file_exists = os.path.exists(csv_output_path)\n",
    "    with open(csv_output_path, mode='a' if file_exists else 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        if not file_exists:\n",
    "            csv_writer.writerow([\"ID\", \"file_name\", \"total_green\", \"total_white\", \"green_exclusive\", \"white_exclusive\", \"overlap\", \"total_area\"])\n",
    "\n",
    "        # Process \"growth\" images using user-defined HSV ranges\n",
    "        for file_name in growth_images:\n",
    "            image_path = os.path.join(target_folder, file_name)\n",
    "            if os.path.exists(image_path):\n",
    "                # Convert GIMP HSV bounds to OpenCV\n",
    "                opencv_green_bounds = (\n",
    "                    convert_hsv_gimp_to_opencv(gimp_green_bounds[0]),\n",
    "                    convert_hsv_gimp_to_opencv(gimp_green_bounds[1])\n",
    "                )\n",
    "                opencv_white_bounds = (\n",
    "                    convert_hsv_gimp_to_opencv(gimp_white_bounds[0]),\n",
    "                    convert_hsv_gimp_to_opencv(gimp_white_bounds[1])\n",
    "                )\n",
    "                opencv_ignored_bounds = [\n",
    "                    (convert_hsv_gimp_to_opencv(bounds[0]), convert_hsv_gimp_to_opencv(bounds[1]))\n",
    "                    for bounds in gimp_ignored_bounds\n",
    "                ] if gimp_ignored_bounds else []\n",
    "\n",
    "                result = process_single_image(image_path, opencv_green_bounds, opencv_white_bounds, opencv_ignored_bounds, output_folder)\n",
    "                if result:\n",
    "                    # Write the updated result to CSV\n",
    "                    csv_writer.writerow(result)\n",
    "                    print(f\"Processed (Growth) {file_name}: Total Green={result[2]}, Total White={result[3]}, Green Exclusive={result[4]}, White Exclusive={result[5]}, Overlap={result[6]}, Total Area={result[7]}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Image not found: {file_name}\")\n",
    "\n",
    "        # Process \"no_growth\" images using default color ranges\n",
    "        for file_name in no_growth_images:\n",
    "            image_path = os.path.join(target_folder, file_name)\n",
    "            if os.path.exists(image_path):\n",
    "                # Use default HSV values\n",
    "                opencv_green_bounds = (\n",
    "                    convert_hsv_gimp_to_opencv((1, 1, 1)),\n",
    "                    convert_hsv_gimp_to_opencv((1, 1, 1))\n",
    "                )\n",
    "                opencv_white_bounds = (\n",
    "                    convert_hsv_gimp_to_opencv((1, 1, 1)),\n",
    "                    convert_hsv_gimp_to_opencv((1, 1, 1))\n",
    "                )\n",
    "                opencv_ignored_bounds = []\n",
    "\n",
    "                result = process_single_image(image_path, opencv_green_bounds, opencv_white_bounds, opencv_ignored_bounds, output_folder)\n",
    "                if result:\n",
    "                    # Write the updated result to CSV\n",
    "                    csv_writer.writerow(result)\n",
    "                    print(f\"Processed (No Growth) {file_name}: Green={result[2]}, White={result[3]}, Overlap={result[4]}, Total={result[7]}\")\n",
    "            else:\n",
    "                print(f\"Image not found: {file_name}\")\n",
    "\n",
    "    print(\"Processing completed. Results saved to\", csv_output_path)\n",
    "\n",
    "\n",
    "# Define paths\n",
    "target_folder = \"D:/internship_images/spruce_sites/Site 8/normalized\"  # Folder containing the images\n",
    "output_folder = \"D:/internship_images/extras/Site 8/lichens\"      # Folder to save segmented images\n",
    "csv_output_path = \"lichen_data_exploration/S168_lichen.csv\"   # Output CSV file\n",
    "\n",
    "# Define image lists\n",
    "growth_images = [\"S168-1082.png\", \"S168-1083.png\", \"S168-1084.png\", \"S168-1085.png\"] # List of images with growth\n",
    "\n",
    "no_growth_images = []    # List of images without growth0\n",
    "\n",
    "# Define HSV bounds in GIMP format (adjust as needed)\n",
    "gimp_green_bounds = [(43, 28, 42), (60, 47, 82)]   # green range in GIMP HSV\n",
    "gimp_white_bounds = [(40, 6, 77), (60, 20, 100)]   # white range in GIMP HSV\n",
    "gimp_ignored_bounds = []  # Ignored range in GIMP HSV\n",
    "\n",
    "# Run processing\n",
    "process_images(target_folder, growth_images, no_growth_images, output_folder, csv_output_path, gimp_green_bounds, gimp_white_bounds, gimp_ignored_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc17d1-9a7a-4cb0-82fd-53ae1d415ac6",
   "metadata": {},
   "source": [
    "# Fix and Integrate Corrected Lichen Areas into Main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842baeb0-4ad2-479b-a15b-f735b4c54e15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fix for Double counted overlap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1515e4-83ef-40fa-bed6-85d6ffdf6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv = \"lichen_data_exploration/specific_correction/results_overlap_problem.csv\"\n",
    "output_csv = \"lichen_data_exploration/specific_correction/results_overlap_problem_fixed.csv\"\n",
    "\n",
    "# Load data set\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Compute correc exclusives\n",
    "df[\"green_exclusive\"] = df[\"total_green\"] - df[\"overlap\"]\n",
    "df[\"white_exclusive\"] = df[\"total_white\"] - df[\"overlap\"]\n",
    "\n",
    "# Correct total_area by substracting twice\n",
    "df[\"total_area\"] = df[\"total_area\"] - 2 * df[\"overlap\"]\n",
    "\n",
    "# Reorder columns\n",
    "df = df[[\"ID\", \"file_name\",\n",
    "        \"total_green\",\"total_white\",\n",
    "        \"green_exclusive\", \"white_exclusive\",\n",
    "        \"overlap\", \"total_area\"\n",
    "]]\n",
    "\n",
    "# Save fixed dataset\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Corrected data saved as {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc663eb7-ebd1-4a3f-9322-a8446f5b1a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Count IDs to spot duplicates in lichen area sets before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87811e-f4eb-4ca3-9f6e-65b5884b3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data sets\n",
    "df1 = pd.read_csv(\"lichen_data_exploration/specific_correction/results_overlap_problem_fixed_removed_dupes.csv\")\n",
    "df2 = pd.read_csv(\"lichen_data_exploration/specific_correction/results_corrected.csv\")\n",
    "\n",
    "# Count ID occurences in each data est\n",
    "counts1 = df1['ID'].value_counts().rename(\"count_overlap_fixed\")\n",
    "counts2 = df2['ID'].value_counts().rename(\"counts_results_corrected\")\n",
    "\n",
    "# Merge counts into one dataframe\n",
    "merged_counts = pd.concat([counts1, counts2], axis=1).fillna(0).astype(int)\n",
    "\n",
    "# Flag IDs where counts are not equal to 4\n",
    "problem_ids = merged_counts[(merged_counts['count_overlap_fixed'] != 4) | (merged_counts['counts_results_corrected'] != 4)]\n",
    "\n",
    "# Export to Excel\n",
    "with pd.ExcelWriter(\"lichen_data_exploration/specific_correction/ID_counts_check.xlsx\") as writer:\n",
    "    merged_counts.to_excel(writer, sheet_name=\"All_ID_Counts\")\n",
    "    problem_ids.to_excel(writer, sheet_name=\"Problem_IDs\")\n",
    "\n",
    "print(\"Exported to ID_counts_check.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e9195-ee74-4fe1-bf30-ed4eb699e17b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Merge Cleaned and Corrected Lichen Area data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad92864-08d7-4685-affc-ad85cc9a79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned data sets\n",
    "\n",
    "df1 = pd.read_csv(\"lichen_data_exploration/specific_correction/results_overlap_problem_fixed_removed_dupes.csv\")\n",
    "df2 = pd.read_csv(\"lichen_data_exploration/specific_correction/results_corrected.csv\")\n",
    "output = \"lichen_data_exploration/specific_correction/merged_corrected_lichen_area.csv\"\n",
    "# Ensure there is no ID overlap\n",
    "overlap = set(df1['ID']).intersection(set(df2['ID']))\n",
    "if overlap:\n",
    "    print(\" Warning: These IDs appear in both datasets:\", overlap)\n",
    "\n",
    "# Stack datasets\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "merged_df.to_csv(output, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved as {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb581315-d89b-4490-8e4c-10eada81bf43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Update Main Dataset with Corrected Lichen Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3049ca-f5dc-4259-afde-1f076c82e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "main_df = pd.read_csv(\"cleaned_data/updated_lichen_area_dataset_prop_3dec.csv\")\n",
    "merged_df = pd.read_csv(\"lichen_data_exploration/specific_correction/merged_corrected_lichen_area.csv\")\n",
    "\n",
    "matching_files = set(main_df['file_name']).intersection(set(merged_df['file_name']))\n",
    "print(f\"Matching file_names: {len(matching_files)}\")\n",
    "print(f\"Non-matching file_names in merged_df: {set(merged_df['file_name']) - set(main_df['file_name'])}\")\n",
    "\n",
    "\n",
    "# Find overlapping columns excluding file_name (used as key) and ID (no need to update)\n",
    "common_cols = list(set(main_df.columns).intersection(merged_df.columns))\n",
    "common_cols.remove(\"file_name\")\n",
    "\n",
    "# Columns to update (excluding 'file_name' and 'ID')\n",
    "cols_to_update = [col for col in merged_df.columns if col not in ['file_name', 'ID']]\n",
    "\n",
    "# Create a mapping dictionary for each column\n",
    "for col in cols_to_update:\n",
    "    mapping = dict(zip(merged_df['file_name'], merged_df[col]))\n",
    "    main_df[col] = main_df['file_name'].map(mapping).combine_first(main_df[col])\n",
    "\n",
    "\n",
    "# Save updated dataset\n",
    "output_main = \"cleaned_data/main_data_corrected_areas_no_proportion.csv\"\n",
    "main_df.to_csv(output_main, index=False)\n",
    "print(f\"Main dataset updated and saved as {output_main}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151054a-2286-489c-9d79-a9d9df930a35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Confirm the updates values by comparing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecf2b8-4277-4a36-a81d-66435f8b8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "main_df = pd.read_csv(\"cleaned_data/main_data_corrected_areas_no_proportion.csv\")\n",
    "merged_df = pd.read_csv(\"lichen_data_exploration/specific_correction/merged_corrected_lichen_area.csv\")\n",
    "\n",
    "# Columns to check (exclude 'file_name' and 'ID' if needed)\n",
    "cols_to_check = [col for col in merged_df.columns if col != 'file_name']\n",
    "\n",
    "# Merge main dataset with merged dataset on file_name for comparison\n",
    "check_df = pd.merge(\n",
    "    main_df[['file_name'] + cols_to_check],\n",
    "    merged_df[['file_name'] + cols_to_check],\n",
    "    on='file_name',\n",
    "    suffixes=('_main', '_merged'),\n",
    "    how='right'\n",
    ")\n",
    "print(check_df)\n",
    "# Find differences\n",
    "differences = []\n",
    "for col in cols_to_check:\n",
    "    diff = check_df[check_df[f\"{col}_main\"] != check_df[f\"{col}_merged\"]]\n",
    "    if not diff.empty:\n",
    "        differences.append((col, diff[['file_name', f\"{col}_main\", f\"{col}_merged\"]]))\n",
    "\n",
    "# Print differences\n",
    "if differences:\n",
    "    print(\" Differences found:\")\n",
    "    for col, diff in differences:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(diff)\n",
    "else:\n",
    "    print(\" All columns match perfectly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39d118-8bd4-446f-8b24-ecbdf625bad3",
   "metadata": {},
   "source": [
    "# Bark Thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a3a78-55e0-4052-a90b-91526240fb65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyse Image and Plot Thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc01de9-c6fd-41f3-81f5-e5be36b5f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "def compute_bark_thickness(image_path):\n",
    "    # Load the image and mask\n",
    "    image = imread(image_path)\n",
    "    if image.shape[-1] == 4:\n",
    "        rgb_image = image[..., :3]\n",
    "        mask = image[..., 3]  # Assuming the 4th channel is the mask\n",
    "    else: \n",
    "        raise ValueError(\"Image does not have a mask channel\")\n",
    "\n",
    "    # Convert RGB to HSV and extract the value component\n",
    "    hsv_image = rgb2hsv(rgb_image)\n",
    "    value_component = hsv_image[..., 2]\n",
    "\n",
    "    # Initialize matrix for value along radii\n",
    "    value_matrix = -1 * np.ones((3601, 629))\n",
    "\n",
    "    # Calculate the center of the mask\n",
    "    center_x, center_y = center_of_mass(mask)\n",
    "\n",
    "    # Loop through angles to compute radii values\n",
    "    idx = 0\n",
    "    for angle in np.arange(0, 2 * np.pi, 0.01):\n",
    "        value_vector = return_value_component_along_radius(value_component, mask, angle, center_x, center_y)\n",
    "        \n",
    "        # Reverse the vector (from center-to-edge to edge-to-center)\n",
    "        value_vector = value_vector[::-1]\n",
    "\n",
    "        # Find the bark's edge (non-masked part)\n",
    "        i = 0\n",
    "        while i < len(value_vector) and value_vector[i] == -1:\n",
    "            i += 1\n",
    "\n",
    "        # Remove parts outside the mask\n",
    "        value_vector = value_vector[i:]\n",
    "\n",
    "        # Save the processed vector into the matrix\n",
    "        value_matrix[:len(value_vector), idx] = value_vector\n",
    "        idx += 1\n",
    "\n",
    "    # Replace -1 values with NaN for nanmedian to ignore them\n",
    "    value_matrix[value_matrix == -1] = np.nan\n",
    "\n",
    "    # Plot the median curve over all radii; limit x-axis to between 0..500\n",
    "    plt.figure(1)\n",
    "    \n",
    "    # Compute the nanmedian along valid radii\n",
    "    median_curve = np.nanmedian(value_matrix, axis=1)\n",
    "    \n",
    "    plt.plot(median_curve, linewidth=2)\n",
    "    plt.xlim([0, 500])\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    \n",
    "    # Set linewidth for the plot frame spines\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "    \n",
    "    # Set axis labels and title\n",
    "    plt.title(image_path.split('/')[-1].replace('_', '-'))\n",
    "    plt.ylabel('Median over radii (HSV Value)')\n",
    "    plt.xlabel('Distance from outer edge of bark')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the value matrix image\n",
    "    plt.figure(2)\n",
    "    plt.imshow(value_matrix.T, aspect='auto', cmap='gray')\n",
    "    plt.xlim([0, 1600])\n",
    "    plt.show()\n",
    "\n",
    "def return_value_component_along_radius(value_component, mask, angle, center_x, center_y):\n",
    "    width, height = value_component.shape\n",
    "    longest_radius = int(np.floor(np.sqrt(width**2 + height**2) / 2))\n",
    "    value_vector = -1 * np.ones(longest_radius + 1)\n",
    "\n",
    "    # Traverse from center to edge\n",
    "    for c in range(longest_radius + 1):\n",
    "        x = int(center_x + c * np.cos(angle))\n",
    "        y = int(center_y + c * np.sin(angle))\n",
    "\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            if mask[x, y] == 255:  # Inside mask area\n",
    "                value_vector[c] = value_component[x, y]\n",
    "    \n",
    "    return value_vector\n",
    "\n",
    "# Call the function with the image path\n",
    "compute_bark_thickness(\"bark_thickness/images/P103-33_transparent_2.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465eef7-ca08-4e93-93dd-a9645cf093a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bark Thickness - Derivative Aproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f54fa-06d7-4fb5-904c-fb1b57489b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "def compute_bark_thickness(image_path, save_dir):\n",
    "    \"\"\"Computes bark thickness and saves the plot.\"\"\"\n",
    "    image = imread(image_path)\n",
    "    rgb_image = image[..., :3]\n",
    "    mask = (image[..., 3] > 0).astype(np.uint8) * 255  # Alpha channel, used later for determining valid image area\n",
    "    \n",
    "    hsv_image = rgb2hsv(rgb_image)\n",
    "    value_component = hsv_image[..., 2]\n",
    "\n",
    "    value_matrix = -1 * np.ones((3601, 629))\n",
    "    center_x, center_y = image.shape[0] / 2, image.shape[1] / 2\n",
    "    \n",
    "    idx = 0\n",
    "    for angle in np.arange(0, 2 * np.pi, 0.01):\n",
    "        value_vector = return_value_component_along_radius(value_component, mask, angle, center_x, center_y)\n",
    "        value_vector = value_vector[::-1]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(value_vector) and value_vector[i] == -1:\n",
    "            i += 1\n",
    "\n",
    "        value_vector = value_vector[i:]\n",
    "        value_matrix[:len(value_vector), idx] = value_vector\n",
    "        idx += 1\n",
    "\n",
    "    value_matrix[value_matrix == -1] = np.nan\n",
    "    median_curve = np.nanmedian(value_matrix, axis=1)\n",
    "    \n",
    "    # Compute derivative analysis\n",
    "    thickness_ranges = np.arange(20, 501, 5)\n",
    "    table_of_derivatives = np.zeros((500, len(thickness_ranges)))\n",
    "    \n",
    "    for i, t_range in enumerate(thickness_ranges):\n",
    "        x = np.arange(1, t_range + 1)\n",
    "        poly_fit = poly.Polynomial.fit(x, median_curve[:t_range], 5)\n",
    "        derivative_values = poly_fit.deriv()(x)\n",
    "        table_of_derivatives[:len(derivative_values), i] = derivative_values\n",
    "    \n",
    "    smoothed_mean_derivative = np.convolve(np.nanmean(table_of_derivatives, axis=1), np.ones(5)/5, mode='valid')\n",
    "    thickness = np.argmax(smoothed_mean_derivative)\n",
    "    peak_derivative_value = np.max(smoothed_mean_derivative)\n",
    "\n",
    "    # Plot and save median curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(median_curve, color=\"blue\", linestyle=\"dashed\", label=\"Median Curve\", alpha=0.6)\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Median over radii (HSV Value)\", fontsize=14)\n",
    "    plt.title(f\"Median Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "    \n",
    "    plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_curve.png\"\n",
    "    plot_path = os.path.join(save_dir, plot_filename)\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot and save smoothed derivative curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(smoothed_mean_derivative, linewidth=2, color='red', label='Smoothed Mean Derivative')\n",
    "    plt.axvline(x=thickness, color='black', linestyle='--', label=f'Thickness ({thickness})')\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Smoothed Mean Derivative\", fontsize=14)\n",
    "    plt.title(f\"Smoothed Derivative Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "    \n",
    "    derivative_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_derivative_curve.png\"\n",
    "    derivative_plot_path = os.path.join(save_dir, derivative_plot_filename)\n",
    "    plt.savefig(derivative_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    return thickness, peak_derivative_value, os.path.basename(image_path), plot_path, derivative_plot_path\n",
    "\n",
    "def return_value_component_along_radius(value_component, mask, angle, center_x, center_y):\n",
    "    width, height = value_component.shape\n",
    "    longest_radius = int(np.floor(np.sqrt(width**2 + height**2) / 2))\n",
    "    value_vector = -1 * np.ones(longest_radius + 1)\n",
    "\n",
    "    for c in range(longest_radius + 1):\n",
    "        x = int(center_x + c * np.cos(angle))\n",
    "        y = int(center_y + c * np.sin(angle))\n",
    "\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            if mask[x, y] == 255:\n",
    "                value_vector[c] = value_component[x, y]\n",
    "    \n",
    "    return value_vector\n",
    "\n",
    "def extract_id(filename):\n",
    "    match = re.match(r\"([A-Za-z0-9]+)-\", filename)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "def process_folder(input_folder, output_csv, plot_folder):\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    results_by_id = {}\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            thickness, peak_derivative_value, image_name, plot_path, derivative_plot_path = compute_bark_thickness(image_path, plot_folder)\n",
    "\n",
    "            if thickness is not None:\n",
    "                image_id = extract_id(filename)\n",
    "                if image_id not in results_by_id:\n",
    "                    results_by_id[image_id] = []\n",
    "                results_by_id[image_id].append((image_name, thickness))\n",
    "\n",
    "    for image_id in results_by_id:\n",
    "        results_by_id[image_id].sort(key=lambda x: x[1])\n",
    "\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        f.write(\"ID,image_name,thickness,peak_derivative_value\\n\")\n",
    "        for image_id, images in results_by_id.items():\n",
    "            for image_name, thickness in images:\n",
    "                f.write(f\"{image_id},{image_name},{thickness}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "    print(f\"Plots saved in {plot_folder}\")\n",
    "\n",
    "\n",
    "input_folder = \"bark_thickness/contrast_test/ends\"\n",
    "output_csv = \"bark_thickness/test_results_3.csv\"\n",
    "plot_folder = \"bark_thickness/plots3\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder, output_csv, plot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3a1d2-b63e-4e24-a076-a6e82b3ea5b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bark Thickness Min or Max Derivative - Transformed Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a56c5d-f1f2-493d-b0e0-f7b79824ebef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "def compute_bark_thickness(image_path, save_dir):\n",
    "    \"\"\"Computes bark thickness and saves multiple plots including overlay on transformed image.\"\"\"\n",
    "    image = imread(image_path)\n",
    "    rgb_image = image[..., :3]\n",
    "    mask = (image[..., 3] > 0).astype(np.uint8) * 255  # Alpha channel as mask\n",
    "    \n",
    "    hsv_image = rgb2hsv(rgb_image)\n",
    "    \"\"\"Select appropriate component to be used in computation Saturation or Value.\"\"\"\n",
    "    component = hsv_image[..., 2] # Value\n",
    "    #component = hsv_image[..., 1] # Saturation\n",
    "    \n",
    "    value_matrix = -1 * np.ones((3601, 629))\n",
    "    center_x, center_y = image.shape[0] / 2, image.shape[1] / 2\n",
    "    \n",
    "    idx = 0\n",
    "    for angle in np.arange(0, 2 * np.pi, 0.01): # Default 0.01 steps\n",
    "        value_vector = return_component_along_radius(component, mask, angle, center_x, center_y)\n",
    "        value_vector = value_vector[::-1]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(value_vector) and value_vector[i] == -1:\n",
    "            i += 1\n",
    "\n",
    "        value_vector = value_vector[i:]\n",
    "        value_matrix[:len(value_vector), idx] = value_vector\n",
    "        idx += 1\n",
    "\n",
    "    value_matrix[value_matrix == -1] = np.nan\n",
    "    median_curve = np.nanmedian(value_matrix, axis=1)\n",
    "    \n",
    "    # Compute derivative analysis\n",
    "    thickness_ranges = np.arange(20, 306, 5)\n",
    "    table_of_derivatives = np.zeros((500, len(thickness_ranges)))\n",
    "    \n",
    "    for i, t_range in enumerate(thickness_ranges):\n",
    "        x = np.arange(1, t_range + 1)\n",
    "        poly_fit = poly.Polynomial.fit(x, median_curve[:t_range], 5)\n",
    "        derivative_values = poly_fit.deriv()(x)\n",
    "        table_of_derivatives[:len(derivative_values), i] = derivative_values\n",
    "    \n",
    "    smoothed_mean_derivative = np.convolve(np.nanmean(table_of_derivatives, axis=1), np.ones(5)/5, mode='valid')\n",
    "\n",
    "    \"\"\"Use variables below if using Value component of HSV.\"\"\"\n",
    "    thickness = np.argmax(smoothed_mean_derivative)\n",
    "    peak_derivative_value = np.max(smoothed_mean_derivative)\n",
    "    \n",
    "    \"\"\"Use variables below if using Satuation component of HSV.\"\"\"\n",
    "    #thickness = np.argmin(smoothed_mean_derivative)\n",
    "    #peak_derivative_value = np.min(smoothed_mean_derivative)\n",
    "        \n",
    "    ### Save Median Curve Plot ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(median_curve, color=\"blue\", linestyle=\"dashed\", label=\"Median Curve\", alpha=0.6)\n",
    "    plt.axvline(x=thickness, color='red', linestyle='--', label=f'Thickness ({thickness})')\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Median over radii (HSV Value)\", fontsize=14)\n",
    "    plt.title(f\"Median Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    median_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_median_curve.png\"\n",
    "    median_plot_path = os.path.join(save_dir, median_plot_filename)\n",
    "    plt.savefig(median_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    ### Save Smoothed Derivative Curve ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(smoothed_mean_derivative, linewidth=2, color='red', label='Smoothed Mean Derivative')\n",
    "    plt.axvline(x=thickness, color='blue', linestyle='--', label=f'Thickness ({thickness})')\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Smoothed Mean Derivative\", fontsize=14)\n",
    "    plt.title(f\"Smoothed Derivative Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    derivative_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_derivative_curve.png\"\n",
    "    derivative_plot_path = os.path.join(save_dir, derivative_plot_filename)\n",
    "    plt.savefig(derivative_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    ### Save Overlay of Thickness Estimate on Transformed Image ###\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot the transformed bark image (without exporting it separately)\n",
    "    ax1.imshow(value_matrix.T, cmap=\"gray\", interpolation=\"nearest\", aspect='auto')\n",
    "    ax1.set_title(f\"Transformed Image with Estimated Thickness ({os.path.basename(image_path)})\")\n",
    "    #ax1.set_xlabel(\"Radius\")\n",
    "    #ax1.set_ylabel(\"Distance from Outer Edge\")\n",
    "    ax1.set_xlim([0, 1400])  # Ensuring proper scaling\n",
    "\n",
    "    # Overlay only the estimated thickness line\n",
    "    ax1.axvline(x=thickness, color='red', linestyle='--', linewidth=2, label=f\"Estimated Thickness ({thickness})\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Save the overlay plot\n",
    "    overlay_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_overlay_transformed.png\"\n",
    "    overlay_plot_path = os.path.join(save_dir, overlay_plot_filename)\n",
    "    plt.savefig(overlay_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return thickness, peak_derivative_value, os.path.basename(image_path), median_plot_path, derivative_plot_path, overlay_plot_path\n",
    "\n",
    "def return_component_along_radius(component, mask, angle, center_x, center_y):\n",
    "    width, height = component.shape\n",
    "    longest_radius = int(np.floor(np.sqrt(width**2 + height**2) / 2))\n",
    "    component_vector = -1 * np.ones(longest_radius + 1)\n",
    "\n",
    "    for r in range(longest_radius + 1):\n",
    "        x = int(center_x + r * np.cos(angle))\n",
    "        y = int(center_y + r * np.sin(angle))\n",
    "\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            if mask[x, y] == 255:\n",
    "                component_vector[r] = component[x, y]\n",
    "    \n",
    "    return component_vector\n",
    "\n",
    "def extract_id(filename):\n",
    "    match = re.match(r\"([A-Za-z0-9]+)-\", filename)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "def process_folder(input_folder, output_csv):\n",
    "    print(\" Starting bark thickness analysis...\")  # Start message\n",
    "    results_by_id = {}\n",
    "\n",
    "    for subdir, _, files in os.walk(input_folder):\n",
    "        if os.path.basename(subdir) != \"ends\":  \n",
    "            continue  # Skip folders that are not the target folder\n",
    "\n",
    "        png_files = [f for f in files if f.endswith(\".png\")]\n",
    "        if not png_files:\n",
    "            continue  # Skip if no PNG files are found in the target folder\n",
    "\n",
    "        parent_folder = os.path.dirname(subdir)  # Get the parent folder of 'target'\n",
    "        plot_folder = os.path.join(parent_folder, \"plots\")  # Save plots in the parent folder\n",
    "        os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n Processing target folder: {subdir} ({len(png_files)} images)\")\n",
    "\n",
    "        for filename in png_files:\n",
    "            image_path = os.path.join(subdir, filename)\n",
    "            print(f\"   Processing image: {filename}...\")\n",
    "\n",
    "            try:\n",
    "                thickness, peak_derivative_value, image_name, median_plot_path, derivative_plot_path, overlay_plot_path = compute_bark_thickness(image_path, plot_folder)\n",
    "\n",
    "                if thickness is not None:\n",
    "                    image_id = extract_id(filename)\n",
    "                    if image_id not in results_by_id:\n",
    "                        results_by_id[image_id] = []\n",
    "                    results_by_id[image_id].append((image_name, thickness))\n",
    "                    \n",
    "                    print(f\"     Thickness computed: {thickness} (Saved to {plot_folder})\")\n",
    "                else:\n",
    "                    print(f\"     Skipping {filename} (No valid thickness found)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing {filename}: {e}\")\n",
    "\n",
    "    # Sort results by thickness\n",
    "    for image_id in results_by_id:\n",
    "        results_by_id[image_id].sort(key=lambda x: x[1])\n",
    "\n",
    "    # Write results to CSV\n",
    "    print(\"\\n Saving results to CSV...\")\n",
    "    write_header = not os.path.exists(output_csv) or os.stat(output_csv).st_size == 0  # Check if file is new or empty\n",
    "\n",
    "    with open(output_csv, \"a\") as f:\n",
    "        if write_header:\n",
    "            f.write(\"ID,file_name,thickness\\n\")  # Write header only once\n",
    "    \n",
    "        for image_id, images in results_by_id.items():\n",
    "            for image_name, thickness in images:\n",
    "                f.write(f\"{image_id},{image_name.replace('.png', '')},{thickness}\\n\")\n",
    "\n",
    "input_folder = 'bark_thickness/contrast_test'\n",
    "output_csv = \"bark_thickness/contrast_test_extended_max_V.csv\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8ecf8-1df7-408a-94ee-540466e155a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bark Thickness Vmax - Smin Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace4922-d614-4f51-a539-d4bb21ea7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "def compute_bark_thickness(image_path, save_dir, save_all_plots=False):\n",
    "    image = imread(image_path)\n",
    "    rgb_image = image[..., :3]\n",
    "    mask = (image[..., 3] > 0).astype(np.uint8) * 255\n",
    "    hsv_image = rgb2hsv(rgb_image)\n",
    "    center_x, center_y = image.shape[0] / 2, image.shape[1] / 2\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for mode, component_label in zip([2, 1], ['value', 'saturation']):\n",
    "        component = hsv_image[..., mode]\n",
    "        value_matrix = -1 * np.ones((3601, 629))\n",
    "\n",
    "        idx = 0\n",
    "        for angle in np.arange(0, 2 * np.pi, 0.01):\n",
    "            value_vector = return_component_along_radius(component, mask, angle, center_x, center_y)\n",
    "            value_vector = value_vector[::-1]\n",
    "            i = 0\n",
    "            while i < len(value_vector) and value_vector[i] == -1:\n",
    "                i += 1\n",
    "            value_vector = value_vector[i:]\n",
    "            value_matrix[:len(value_vector), idx] = value_vector\n",
    "            idx += 1\n",
    "\n",
    "        value_matrix[value_matrix == -1] = np.nan\n",
    "        median_curve = np.nanmedian(value_matrix, axis=1)\n",
    "\n",
    "        # Check for all-NaN\n",
    "        if np.all(np.isnan(median_curve[:300])):\n",
    "            print(f\"     Skipping {component_label} computation (insufficient data)\")\n",
    "            results[component_label] = {\n",
    "                'thickness': None,\n",
    "                'peak': None,\n",
    "                'value_matrix': value_matrix.T,\n",
    "                'median_curve': median_curve,\n",
    "                'smoothed_derivative': None\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        thickness_ranges = np.arange(20, 306, 5)\n",
    "        table_of_derivatives = np.full((500, len(thickness_ranges)), np.nan)\n",
    "\n",
    "        for i, t_range in enumerate(thickness_ranges):\n",
    "            x = np.arange(1, t_range + 1)\n",
    "            y = median_curve[:t_range]\n",
    "            if np.any(np.isnan(y)) or len(y) < 6:\n",
    "                continue  # Skip invalid curve sections\n",
    "\n",
    "            try:\n",
    "                poly_fit = poly.Polynomial.fit(x, y, 5)\n",
    "                derivative_values = poly_fit.deriv()(x)\n",
    "                table_of_derivatives[:len(derivative_values), i] = derivative_values\n",
    "            except (ValueError, np.linalg.LinAlgError):\n",
    "                continue  # Skip bad fit\n",
    "\n",
    "        if np.all(np.isnan(table_of_derivatives)):\n",
    "            print(f\"     No valid derivative for {component_label}\")\n",
    "            thickness = None\n",
    "            peak = None\n",
    "            smoothed_mean_derivative = None\n",
    "        else:\n",
    "            smoothed_mean_derivative = np.convolve(np.nanmean(table_of_derivatives, axis=1), np.ones(5)/5, mode='valid')\n",
    "            if component_label == 'value':\n",
    "                thickness = int(np.nanargmax(smoothed_mean_derivative))\n",
    "                peak = float(np.nanmax(smoothed_mean_derivative))\n",
    "            else:\n",
    "                thickness = int(np.nanargmin(smoothed_mean_derivative))\n",
    "                peak = float(np.nanmin(smoothed_mean_derivative))\n",
    "\n",
    "        results[component_label] = {\n",
    "            'thickness': thickness,\n",
    "            'peak': peak,\n",
    "            'value_matrix': value_matrix.T,\n",
    "            'median_curve': median_curve,\n",
    "            'smoothed_derivative': smoothed_mean_derivative\n",
    "        }\n",
    "\n",
    "        # Optional plot exports\n",
    "        if save_all_plots:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(median_curve, color=\"blue\", linestyle=\"dashed\", label=\"Median Curve\", alpha=0.6)\n",
    "            plt.axvline(x=thickness, color='red', linestyle='--', label=f'Thickness ({thickness})')\n",
    "            plt.xlabel(\"Distance from outer edge of bark\")\n",
    "            plt.ylabel(f\"Median over radii (HSV {component_label.title()})\")\n",
    "            plt.title(f\"Median Curve ({component_label}) - {os.path.basename(image_path)}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "            ax1.set_xlim([0, 1400])  # Ensuring proper scaling\n",
    "            plt.savefig(os.path.join(save_dir, f\"{os.path.basename(image_path).replace('.png', '')}_median_curve_{component_label}.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(smoothed_mean_derivative, linewidth=2, color='red')\n",
    "            plt.axvline(x=thickness, color='blue', linestyle='--', label=f'Thickness ({thickness})')\n",
    "            plt.title(f\"Smoothed Derivative ({component_label}) - {os.path.basename(image_path)}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "            ax1.set_xlim([0, 1400])  # Ensuring proper scaling\n",
    "            plt.savefig(os.path.join(save_dir, f\"{os.path.basename(image_path).replace('.png', '')}_derivative_curve_{component_label}.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "    ### Overlay plot with both Value and Saturation thickness ###\n",
    "    if results['value']['thickness'] is not None or results['saturation']['thickness'] is not None:\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.imshow(results['value']['value_matrix'], cmap=\"gray\", aspect='auto')\n",
    "        if results['value']['thickness'] is not None:\n",
    "            ax1.axvline(x=results['value']['thickness'], color='red', linestyle='--', linewidth=2, label=f\"Value ({results['value']['thickness']})\")\n",
    "        if results['saturation']['thickness'] is not None:\n",
    "            ax1.axvline(x=results['saturation']['thickness'], color='blue', linestyle='--', linewidth=2, label=f\"Saturation ({results['saturation']['thickness']})\")\n",
    "        ax1.set_title(f\"Overlay Thickness - {os.path.basename(image_path)}\")\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        overlay_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_overlay_transformed.png\"\n",
    "        overlay_plot_path = os.path.join(save_dir, overlay_plot_filename)\n",
    "        ax1.set_xlim([0, 1400])  # Ensuring proper scaling\n",
    "        plt.savefig(overlay_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        overlay_plot_path = \"None\"\n",
    "\n",
    "\n",
    "    return (results['value']['thickness'], results['value']['peak'],\n",
    "            results['saturation']['thickness'], results['saturation']['peak'],\n",
    "            os.path.basename(image_path), overlay_plot_path)\n",
    "\n",
    "\n",
    "def return_component_along_radius(component, mask, angle, center_x, center_y):\n",
    "    width, height = component.shape\n",
    "    longest_radius = int(np.floor(np.sqrt(width**2 + height**2) / 2))\n",
    "    component_vector = -1 * np.ones(longest_radius + 1)\n",
    "\n",
    "    for r in range(longest_radius + 1):\n",
    "        x = int(center_x + r * np.cos(angle))\n",
    "        y = int(center_y + r * np.sin(angle))\n",
    "\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            if mask[x, y] == 255:\n",
    "                component_vector[r] = component[x, y]\n",
    "    \n",
    "    return component_vector\n",
    "\n",
    "def extract_id(filename):\n",
    "    match = re.match(r\"([A-Za-z0-9]+)-\", filename)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "def process_folder(input_folder, output_csv):\n",
    "    print(\" Starting bark thickness analysis...\")  # Start message\n",
    "    results_by_id = {}\n",
    "\n",
    "    for subdir, _, files in os.walk(input_folder):\n",
    "        if os.path.basename(subdir) != \"ends\":  \n",
    "            continue  # Skip folders that are not the target folder\n",
    "\n",
    "        png_files = [f for f in files if f.endswith(\".png\")]\n",
    "        if not png_files:\n",
    "            continue  # Skip if no PNG files are found in the target folder\n",
    "\n",
    "        parent_folder = os.path.dirname(subdir)  # Get the parent folder of 'target'\n",
    "        plot_folder = os.path.join(parent_folder, \"plots\")  # Save plots in the parent folder\n",
    "        os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n Processing target folder: {subdir} ({len(png_files)} images)\")\n",
    "\n",
    "        for filename in png_files:\n",
    "            image_path = os.path.join(subdir, filename)\n",
    "            print(f\"   Processing image: {filename}...\")\n",
    "\n",
    "            try:\n",
    "                thickness_val, peak_val, thickness_sat, peak_sat, image_name, overlay_plot_path = compute_bark_thickness(image_path, plot_folder, save_all_plots=False)\n",
    "\n",
    "                if thickness_val is not None or thickness_sat is not None:\n",
    "                    image_id = extract_id(filename)\n",
    "                    if image_id not in results_by_id:\n",
    "                        results_by_id[image_id] = []\n",
    "                    results_by_id[image_id].append((image_name, thickness_val, peak_val, thickness_sat, peak_sat))\n",
    "                    \n",
    "                    print(f\"     Thickness computed: Value={thickness_val}, Saturation={thickness_sat} (Saved to {plot_folder})\")\n",
    "                else:\n",
    "                    print(f\"     Skipping {filename} (No valid thickness found)\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing {filename}: {e}\")\n",
    "\n",
    "    # Sort results by thickness\n",
    "    for image_id in results_by_id:\n",
    "        results_by_id[image_id].sort(key=lambda x: x[1])\n",
    "\n",
    "    # Write results to CSV\n",
    "    print(\"\\n Saving results to CSV...\")\n",
    "    write_header = not os.path.exists(output_csv) or os.stat(output_csv).st_size == 0  # Check if file is new or empty\n",
    "\n",
    "    with open(output_csv, \"a\") as f:\n",
    "        if write_header:\n",
    "            f.write(\"ID,file_name,thickness_value,thickness_saturation\\n\")\n",
    "            #f.write(\"ID,file_name,thickness_value,peak_value,thickness_saturation,peak_saturation\\n\")  # Write header only once\n",
    "    \n",
    "        for image_id, images in results_by_id.items():\n",
    "            for image_name, thickness_val, peak_val, thickness_sat, peak_sat in images:\n",
    "                f.write(f\"{image_id},{image_name.replace('.png', '')},{thickness_val or ''},{thickness_sat or ''}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "input_folder = 'bark_thickness\\contrast_correction\\processed'\n",
    "output_csv = \"thickness_double_test2_analysis.csv\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020956d-008a-47d4-b84f-707d15d21004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(input_folder, output_csv, plot_folder):\n",
    "    os.makedirs(plot_folder, exist_ok=True)\n",
    "    results_by_id = {}\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            thickness, peak_derivative_value, image_name, median_plot_path, derivative_plot_path, overlay_plot_path = compute_bark_thickness(image_path, plot_folder)\n",
    "\n",
    "            if thickness is not None:\n",
    "                image_id = extract_id(filename)\n",
    "                if image_id not in results_by_id:\n",
    "                    results_by_id[image_id] = []\n",
    "                results_by_id[image_id].append((image_name, thickness))\n",
    "\n",
    "    for image_id in results_by_id:\n",
    "        results_by_id[image_id].sort(key=lambda x: x[1])\n",
    "\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        f.write(\"ID,image_name,thickness,peak_derivative_value\\n\")\n",
    "        for image_id, images in results_by_id.items():\n",
    "            for image_name, thickness in images:\n",
    "                f.write(f\"{image_id},{image_name},{thickness}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "    print(f\"Plots saved in {plot_folder}\")\n",
    "\n",
    "\n",
    "input_folder = \"bark_thickness/non_normal\"\n",
    "output_csv = \"bark_thickness/test_results_non_normal.csv\"\n",
    "plot_folder = \"bark_thickness/plots_non_normal\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder, output_csv, plot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193d729-3939-4afc-b938-fadef5de6fc7",
   "metadata": {},
   "source": [
    "# Bark Thickness Conversion from Pixel to Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64074bbd-eebe-4818-9fbe-25c41b3d5e65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract Diameter Information from Data Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae89c7-8b58-4b4f-9c49-2edc6b4f8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"generated_intermediate_data/Log pictures.xlsx\")\n",
    "\n",
    "# Specify the column names exactly as they appear in your sheet\n",
    "col1 = \"Log_ID\"\n",
    "col2 = \"Photo_top\"\n",
    "col3 = \"Diameter_cm\"\n",
    "\n",
    "# Combine the first two columns with a hyphen\n",
    "df[\"file_name\"] = df[col1].astype(str) + \"-\" + df[col2].astype(str)\n",
    "\n",
    "# Convert column 3 text to lowercase\n",
    "df[col3] = df[col3].astype(str).str.lower()   # can use .str.upper() for uppercase\n",
    "\n",
    "# Create the final dataset\n",
    "final_df = df[[col1, \"file_name\", col3]]\n",
    "\n",
    "# Save the result\n",
    "final_df.to_excel(\"diameter_data.xlsx\", index=False)\n",
    "# or for CSV:\n",
    "final_df.to_csv(\"diameter_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3092b-62b1-4386-a413-5af8739c1e49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exract Date information and Assign Groups\n",
    "\n",
    "Since there is no standart for photography sessions, to correct for camera distance, images taken on same day will be processed together in bark thickness conversion from pixel to metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81211cb1-9c70-4d00-99dd-fb8611d89624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the two main folders\n",
    "main_folders = [\"D:\\internship_images\\pine_sites\", \"D:\\internship_images\\spruce_sites\", \"D:\\internship_images\\extras\\pine\", \"D:\\internship_images\\extras\\spruce\"]\n",
    "\n",
    "# Name of the folder that actually contains the images\n",
    "target_folder_name = \"original\" \n",
    "\n",
    "records = []\n",
    "\n",
    "def extract_id(filename):\n",
    "    name = os.path.splitext(filename)[0]  # remove extension\n",
    "    return name.split(\"-\")[0] if \"-\" in name else None\n",
    "\n",
    "def get_exif_date(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        exif_data = image._getexif()\n",
    "        if exif_data:\n",
    "            for tag, value in exif_data.items():\n",
    "                tag_name = TAGS.get(tag, tag)\n",
    "                if tag_name in [\"DateTimeOriginal\", \"DateTimeDigitized\", \"DateTime\"]:\n",
    "                    # Format: \"YYYY:MM:DD HH:MM:SS\"\n",
    "                    date_str = value.split(\" \")[0]  # only YYYY:MM:DD\n",
    "                    year, month, day = date_str.split(\":\")\n",
    "                    return f\"{day}-{month}-{year}\"  # day-month-year\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading EXIF from {image_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Walk through the folder structure\n",
    "for folder in main_folders:\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        # Check if target folder name appears in the path\n",
    "        if target_folder_name in root:\n",
    "            for file in files:\n",
    "                if file.lower().endswith((\".jpg\")):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    date_taken = get_exif_date(file_path)\n",
    "            \n",
    "                    # Extract ID from filename\n",
    "                    ID = extract_id(file)\n",
    "            \n",
    "                    # Remove file extension\n",
    "                    file_name_no_ext = os.path.splitext(file)[0]\n",
    "            \n",
    "                    # Store record\n",
    "                    if date_taken:\n",
    "                        records.append([ID, file_name_no_ext, date_taken])\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records, columns=[\"ID\", \"file_name\", \"date\"])\n",
    "\n",
    "if not df.empty:\n",
    "    # Assign group numbers based on unique dates\n",
    "    df[\"group\"] = df.groupby(\"date\").ngroup() + 1\n",
    "    # Save to Excel\n",
    "    df.to_excel(\"photo_dates.xlsx\", index=False)\n",
    "    print(\" Excel file 'photo_dates.xlsx' created successfully!\")\n",
    "else:\n",
    "    print(\" No EXIF dates found. Check folder name or image metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f294d-a002-4b35-9a5f-477a0b9429e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Integrate Assigned group code to Measured Diameter data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb787105-a820-461d-8b4e-b108d86405ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load main dataset\n",
    "df_main = pd.read_csv(\"diameter_lichen_data/final_main_data_date_diameter_per_pixel.csv\")\n",
    "\n",
    "# Load the second dataset (the one with extra info)\n",
    "df_extra = pd.read_csv(\"bark_thickness_filtered_tops.csv\")\n",
    "\n",
    "# Define the common column\n",
    "key_col = \"ID\"   # must exist in both datasets\n",
    "\n",
    "# Define which column(s) you want to bring in from df_extra\n",
    "cols_to_add = [\"thickness\"]\n",
    "\n",
    "# Merge datasets based on the key\n",
    "merged_df = pd.merge(df_main, df_extra[[key_col] + cols_to_add], on=key_col, how=\"left\")\n",
    "\n",
    "# Convert \"group\" column to integer\n",
    "merged_df[\"group\"] = pd.to_numeric(merged_df[\"group\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Save the merged result\n",
    "merged_df.to_excel(\"final_main_data_date_diameter_per_pixel_thickness.xlsx\", index=False)\n",
    "# or CSV:\n",
    "merged_df.to_csv(\"final_main_data_date_diameter_per_pixel_thickness.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada5f5d-81dd-44cd-8ce2-563b3c51c469",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extract Diameter in Pixels using Measured Diameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4e4b7-4bc8-4cb0-83f2-6b75f0ec751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Input data \n",
    "imgPath1 = 'thickness_conversion_samples/S6-696.png'\n",
    "measuredDiam1 = 20\n",
    "imgPath2 = 'thickness_conversion_samples/S220-606.png'\n",
    "measuredDiam2 = 21\n",
    "imgPath3 = 'thickness_conversion_samples/S223-852.png'\n",
    "measuredDiam3 = 22\n",
    "\n",
    "# Hardcoded selection of image to be processed\n",
    "imgPath = imgPath3\n",
    "measuredDiam = measuredDiam3\n",
    "\n",
    "def rotate_image_keep_bounds(image, angle):\n",
    "    \"\"\"Rotate image without cropping.\"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    # rotation matrix\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # compute new bounding dimensions\n",
    "    cos = np.abs(rot_mat[0, 0])\n",
    "    sin = np.abs(rot_mat[0, 1])\n",
    "    new_w = int(h * sin + w * cos)\n",
    "    new_h = int(h * cos + w * sin)\n",
    "\n",
    "    # adjust translation to keep image centered\n",
    "    rot_mat[0, 2] += (new_w / 2) - center[0]\n",
    "    rot_mat[1, 2] += (new_h / 2) - center[1]\n",
    "\n",
    "    # perform rotation\n",
    "    return cv2.warpAffine(image, rot_mat, (new_w, new_h), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "\n",
    "# Read alpha channel (mask) \n",
    "img = cv2.imread(imgPath, cv2.IMREAD_UNCHANGED)  # includes alpha if present\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Could not read {imgPath}\")\n",
    "\n",
    "if img.shape[2] < 4:\n",
    "    raise ValueError(f\"{imgPath} does not have an alpha channel\")\n",
    "\n",
    "alpha = img[:, :, 3]  # alpha channel\n",
    "sz = alpha.shape\n",
    "height = 0\n",
    "diameter = 0\n",
    "\n",
    "# Rotate 0:5:90 degrees and find max side\n",
    "for angle in range(0, 91, 5):\n",
    "    # rotate\n",
    "    h, w = alpha.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = rotate_image_keep_bounds(alpha, angle)\n",
    "\n",
    "    # trim empty rows/cols\n",
    "    cols = np.any(rotated > 0, axis=0)\n",
    "    rows = np.any(rotated > 0, axis=1)\n",
    "    rotated = rotated[np.ix_(rows, cols)]\n",
    "\n",
    "    # check longest side\n",
    "    if rotated.shape[0] > height:\n",
    "        height = rotated.shape[0]\n",
    "        diameter = rotated.shape[1]\n",
    "\n",
    "    if rotated.shape[1] > height:\n",
    "        height = rotated.shape[1]\n",
    "        diameter = rotated.shape[0]\n",
    "\n",
    "# Print results\n",
    "file_name = os.path.basename(imgPath)\n",
    "print(f\"\\n            Image = {file_name}\")\n",
    "print(f\"       Image size = {sz[1]} x {sz[0]}\")  # width x height\n",
    "print(f\"         Diameter = {diameter} pixels (corresponding long side = {height} pixels)\")\n",
    "print(f\"Measured diameter = {measuredDiam} cm\")\n",
    "print(f\"    Pixels per cm = {diameter / measuredDiam:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396142-f959-4be4-8c68-02a897b3b119",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process Folder version - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1652d-7100-4800-8712-1a8887b4eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# rotation helper (keeps full image, no cropping)\n",
    "def rotate_image_keep_bounds(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = ((w - 1) / 2.0, (h - 1) / 2.0)\n",
    "\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    cos = np.abs(rot_mat[0, 0])\n",
    "    sin = np.abs(rot_mat[0, 1])\n",
    "    new_w = int(np.ceil(h * sin + w * cos))\n",
    "    new_h = int(np.ceil(h * cos + w * sin))\n",
    "\n",
    "    rot_mat[0, 2] += (new_w - w) / 2\n",
    "    rot_mat[1, 2] += (new_h - h) / 2\n",
    "\n",
    "    return cv2.warpAffine(image, rot_mat, (new_w, new_h), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "\n",
    "# parameters \n",
    "dataset_path = \"thickness_conversion_samples_2/diameter_data.csv\"              # your dataset with file_name, diameter\n",
    "image_folder = \"thickness_conversion_samples_2\"     # folder where PNGs are stored\n",
    "output_csv = \"rotation_results2.csv\"\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "results = []\n",
    "\n",
    "# process each image\n",
    "for _, row in df.iterrows():\n",
    "    fname = str(row[\"file_name\"]) + \".png\"\n",
    "    measuredDiam = row[\"diameter\"]\n",
    "\n",
    "    imgPath = os.path.join(image_folder, fname)\n",
    "    img = cv2.imread(imgPath, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\" Could not read {imgPath}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    if img.shape[2] < 4:\n",
    "        print(f\" {fname} has no alpha channel, skipping...\")\n",
    "        continue\n",
    "\n",
    "    alpha = img[:, :, 3]\n",
    "    sz = alpha.shape\n",
    "    height = 0\n",
    "    diameter = 0\n",
    "\n",
    "    # rotate 0:5:90\n",
    "    for angle in range(0, 91, 5):\n",
    "        rotated = rotate_image_keep_bounds(alpha, angle)\n",
    "\n",
    "        # trim empty rows/cols\n",
    "        cols = np.any(rotated > 0, axis=0)\n",
    "        rows = np.any(rotated > 0, axis=1)\n",
    "        rotated = rotated[np.ix_(rows, cols)]\n",
    "\n",
    "        # check longest side\n",
    "        if rotated.shape[0] > height:\n",
    "            height = rotated.shape[0]\n",
    "            diameter = rotated.shape[1]\n",
    "\n",
    "        if rotated.shape[1] > height:\n",
    "            height = rotated.shape[1]\n",
    "            diameter = rotated.shape[0]\n",
    "\n",
    "    pixels_per_cm = diameter / measuredDiam\n",
    "\n",
    "    results.append({\n",
    "        \"file_name\": row[\"file_name\"],\n",
    "        #\"image_width\": sz[1],\n",
    "        #\"image_height\": sz[0],\n",
    "        #\"diameter_pixels\": diameter,\n",
    "        #\"long_side_pixels\": height,\n",
    "        \"measured_diameter_cm\": measuredDiam,\n",
    "        \"pixels_per_cm\": round(pixels_per_cm, 1)\n",
    "    })\n",
    "\n",
    "    print(f\" {fname} -> {diameter} px / {measuredDiam} cm = {pixels_per_cm:.1f} px/cm\")\n",
    "\n",
    "# save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(output_csv, index=False)\n",
    "print(f\"\\nResults saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07f974-b06f-4cf7-9d7b-60a6800e7708",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process Subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe53bb-4862-4ed3-97b7-7cfa6d63ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 1. Rotation helper (no cropping, MATLAB-like precision)\n",
    "\n",
    "def rotate_image_keep_bounds(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = ((w - 1) / 2.0, (h - 1) / 2.0)\n",
    "\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    cos = np.abs(rot_mat[0, 0])\n",
    "    sin = np.abs(rot_mat[0, 1])\n",
    "    new_w = int(np.ceil(h * sin + w * cos))\n",
    "    new_h = int(np.ceil(h * cos + w * sin))\n",
    "\n",
    "    rot_mat[0, 2] += (new_w - w) / 2\n",
    "    rot_mat[1, 2] += (new_h - h) / 2\n",
    "\n",
    "    return cv2.warpAffine(image, rot_mat, (new_w, new_h), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "\n",
    "# 2. Configuration\n",
    "\n",
    "main_folder = \"D:\\internship_images\"        # top-level folder containing multiple subfolders\n",
    "dataset_path = \"diameter_date.xlsx\"       # dataset with columns: file_name, diameter\n",
    "output_csv = \"full_diameter_pixel_data.csv\"         # where to save the output\n",
    "\n",
    "\n",
    "# 3. Load dataset\n",
    "\n",
    "df = pd.read_excel(dataset_path)\n",
    "diam_dict = dict(zip(df[\"file_name\"].astype(str), df[\"diameter_cm\"]))\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "# 4. Walk through ALL subfolders and process any 'ends' folder\n",
    "\n",
    "for root, dirs, files in os.walk(main_folder):\n",
    "    # process only folders that are named 'ends'\n",
    "    if os.path.basename(root).lower() != \"ends\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n Processing folder: {root}\")\n",
    "\n",
    "    for fname in files:\n",
    "        if not fname.lower().endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        base = os.path.splitext(fname)[0]  # e.g., \"001\"\n",
    "\n",
    "        if base not in diam_dict:\n",
    "            print(f\" {base} not found in dataset, skipping...\")\n",
    "            continue\n",
    "\n",
    "        measuredDiam = diam_dict[base]\n",
    "        imgPath = os.path.join(root, fname)\n",
    "        img = cv2.imread(imgPath, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\" Could not read {imgPath}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        if len(img.shape) < 3 or img.shape[2] < 4:\n",
    "            print(f\" {fname} has no alpha channel, skipping...\")\n",
    "            continue\n",
    "\n",
    "        alpha = img[:, :, 3]\n",
    "        sz = alpha.shape\n",
    "        height = 0\n",
    "        diameter = 0\n",
    "\n",
    "\n",
    "        # 5. Rotate 090 (every 5) and find longest side\n",
    "\n",
    "        for angle in range(0, 91, 5):\n",
    "            rotated = rotate_image_keep_bounds(alpha, angle)\n",
    "            mask = (rotated > 127).astype(np.uint8)\n",
    "\n",
    "            cols = np.any(mask, axis=0)\n",
    "            rows = np.any(mask, axis=1)\n",
    "            rotated = rotated[np.ix_(rows, cols)]\n",
    "\n",
    "            if rotated.size == 0:\n",
    "                continue\n",
    "\n",
    "            if rotated.shape[0] > height:\n",
    "                height = rotated.shape[0]\n",
    "                diameter = rotated.shape[1]\n",
    "\n",
    "            if rotated.shape[1] > height:\n",
    "                height = rotated.shape[1]\n",
    "                diameter = rotated.shape[0]\n",
    "\n",
    "        pixels_per_cm = diameter / measuredDiam if measuredDiam != 0 else np.nan\n",
    "\n",
    "        # collect results\n",
    "        results.append({\n",
    "            \"file_name\": base,\n",
    "            #\"image_path\": imgPath,\n",
    "            #\"image_width\": sz[1],\n",
    "            #\"image_height\": sz[0],\n",
    "            \"diameter_pixels\": diameter,\n",
    "            \"long_side_pixels\": height,\n",
    "            \"measured_diameter_cm\": measuredDiam,\n",
    "            \"pixels_per_cm\": round(pixels_per_cm, 1)\n",
    "        })\n",
    "\n",
    "        print(f\" {fname}: {diameter}px / {measuredDiam}cm = {pixels_per_cm:.1f} px/cm\")\n",
    "\n",
    "\n",
    "# 6. Save all results\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n Results saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"\\n No valid images processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fba71e-6e62-468d-9944-3cf55c1f0063",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Add ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a8461-d82c-4e10-be44-4d9668deef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(\"full_diameter_pixel_data.csv\")\n",
    "\n",
    "# Extract everything before the first '-' in 'file_name'\n",
    "df['ID'] = df['file_name'].str.split('-', n=1).str[0]\n",
    "\n",
    "# Move the 'ID' column to the first position\n",
    "cols = ['ID'] + [col for col in df.columns if col != 'ID']\n",
    "df = df[cols]\n",
    "\n",
    "# Save or view the result\n",
    "df.to_csv(\"full_diameter_pixel_data_ID.csv\", index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886895b5-688a-4c6a-965c-6c39d23a602c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clean Bark thickness Data, Keeping TOP side measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618b8a74-3391-4d2c-b76b-4cd4686aaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(\"bark_thickness_main/thickness_data.csv\")\n",
    "df2 = pd.read_csv(\"full_diameter_pixel_data_ID.csv\")\n",
    "\n",
    "# Drop rows from df1 where 'file_name' exists in df2\n",
    "df1_filtered = df1[~df1['file_name'].isin(df2['file_name'])].reset_index(drop=True)\n",
    "\n",
    "# Export the filtered dataframe to CSV\n",
    "df1_filtered.to_csv(\"bark_thickness_filtered_tops.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033f079-1683-4928-a485-728c6506432f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Histogram of pixel_per_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9af77b-64bc-40f4-b041-373991bdd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(\"diameter_lichen_data/final_main_data_date_diameter_per_pixel.csv\")\n",
    "\n",
    "# Drop missing diameter values\n",
    "#df = df.dropna(subset=['diameter'])\n",
    "\n",
    "# Define fixed bins (from 70 to 130 in 5-unit steps)\n",
    "bins = np.arange(70, 135, 5)\n",
    "\n",
    "# Prepare subsets\n",
    "datasets = {\n",
    "    \"All_Data\": df,\n",
    "    \"Category_P\": df[df['ID'].astype(str).str.contains('P', case=False, na=False)],\n",
    "    \"Category_S\": df[df['ID'].astype(str).str.contains('S', case=False, na=False)]\n",
    "}\n",
    "\n",
    "# Titles for each figure\n",
    "figure_titles = {\n",
    "    \"All_Data\": \"Histogram of Diameter - All Data\",\n",
    "    \"Category_P\": \"Histogram of Diameter - Category P\",\n",
    "    \"Category_S\": \"Histogram of Diameter - Category S\"\n",
    "}\n",
    "\n",
    "# Plot each dataset\n",
    "for label, subset_df in datasets.items():\n",
    "    groups = subset_df['group'].unique()\n",
    "    num_groups = len(groups)\n",
    "    cols = 4\n",
    "    rows = math.ceil(num_groups / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, g in enumerate(groups):\n",
    "        g_data = subset_df[subset_df['group'] == g]['pixels_per_cm']\n",
    "        count = len(g_data)\n",
    "        mean_val = g_data.mean()\n",
    "\n",
    "        if count > 0:\n",
    "            axes[i].hist(g_data, bins=bins, color='skyblue', edgecolor='black')\n",
    "            axes[i].set_xlim(70, 130)\n",
    "            axes[i].set_title(f\"{g}\\n(n={count}, mean={mean_val:.1f})\", fontsize=10)\n",
    "            axes[i].set_xlabel('pixel/cm')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "        else:\n",
    "            axes[i].set_visible(False)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Add overall figure title\n",
    "    fig.suptitle(figure_titles[label], fontsize=16)\n",
    "    # Adjust layout to make room for suptitle\n",
    "    plt.subplots_adjust(top=0.80)\n",
    "\n",
    "    # Save image\n",
    "    output_path = f\"histograms_{label}.png\"\n",
    "    fig.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\" Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d185b94-d795-4939-af1d-36103d4f5cb8",
   "metadata": {},
   "source": [
    "# Helper Code\n",
    "\n",
    " Related to house keeping task. Data cleaning, file name modifications, missing data and such."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11943cb9-6b44-43d8-9324-ab1c7bf53fd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Find missing row in simplified data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e941d5-2385-40c5-b45f-3a482689f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load two Excel files\n",
    "file_a = \"main_simplified_dataset.xlsx\"  # reference or complete dataset\n",
    "file_b = \"cleaned_data/main_data_corrected_areas_prop.xlsx\"  # dataset to compare\n",
    "\n",
    "df_a = pd.read_excel(file_a)\n",
    "df_b = pd.read_excel(file_b)\n",
    "\n",
    "# Clean and standardize ID column \n",
    "df_a['ID'] = df_a['ID'].astype(str).str.strip()\n",
    "df_b['ID'] = df_b['ID'].astype(str).str.strip()\n",
    "\n",
    "# Step 3: Compare IDs\n",
    "missing_in_b = df_a[~df_a['ID'].isin(df_b['ID'])]  # IDs in A but not in B\n",
    "missing_in_a = df_b[~df_b['ID'].isin(df_a['ID'])]  # IDs in B but not in A\n",
    "\n",
    "# Step 4: Display results directly in notebook\n",
    "print(\" Comparison complete.\\n\")\n",
    "\n",
    "print(f\" IDs in A but missing in B: {len(missing_in_b)}\")\n",
    "display(missing_in_b.head(10))  # show first 10 rows for quick check\n",
    "\n",
    "print(f\"\\n IDs in B but missing in A: {len(missing_in_a)}\")\n",
    "display(missing_in_a.head(10))  # show first 10 rows for quick check\n",
    "\n",
    "# === (Optional) Save to Excel if you also want a file ===\n",
    "# missing_in_b.to_excel(\"missing_in_B.xlsx\", index=False)\n",
    "# missing_in_a.to_excel(\"missing_in_A.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba74461-09db-4734-bcda-dd8dec8f63ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b460c-bc3c-4000-a486-edcfafc55934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('cleaned_data/full_dataset_spruce_pine.csv')\n",
    "\n",
    "# Remove everything after \"_\" in the \"ID\" column\n",
    "#df['ID'] = df['ID'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "# Save the cleaned CSV file\n",
    "#df.to_csv('cleaned_file.csv', index=False)\n",
    "\n",
    "#print(\"IDs cleaned and saved to cleaned_file.csv\")\n",
    "\n",
    "# Load your cleaned CSV and main dataset\n",
    "df_cleaned = df\n",
    "df_main = pd.read_csv('cleaned_data/extras_andmissing_ID_cleaned.csv')\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_merged = pd.concat([df_cleaned, df_main], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset\n",
    "df_merged.to_csv('full_dataset.csv', index=False)\n",
    "\n",
    "print(\"Merging complete. Saved as 'merged_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f7737-0b88-48dc-88e2-ded4b6710894",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sort Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f970ff-83a5-432e-85f3-db9e453ae75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "input_file = \"cleaned_data/full_dataset.csv\"  \n",
    "output_file = \"cleaned_data/sorted_full_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Sort the DataFrame by the 'ID' column\n",
    "df_sorted = df.sort_values(by='ID', ascending=True)\n",
    "\n",
    "# Save the sorted DataFrame back to a CSV file\n",
    "df_sorted.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data sorted by 'ID' and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af93f3-a934-4faa-bf3a-1ac5b4f5f1cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ID Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e4edb-de75-4231-b61e-0ea47d780bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load DataFrame\n",
    "#data = pd.read_csv(\"cleaned_data/spruce_data_cleaned.csv\")\n",
    "data = pd.read_csv(\"final_main_data_date_diameter_per_pixel_thickness.csv\")\n",
    "df_clean_full = pd.DataFrame(data)\n",
    "\n",
    "# Extract the repeating part of the ID (the part before '-')\n",
    "df_clean_full['ID_Part'] = df_clean_full['ID'].str.split('-').str[0]\n",
    "\n",
    "# Count how many times each ID_Part occurs\n",
    "id_counts = df_clean_full['ID_Part'].value_counts()\n",
    "\n",
    "# Identify IDs that don't appear 6 times\n",
    "ids_missing = id_counts[id_counts != 6]\n",
    "\n",
    "#Print the IDs and their occurrence counts (which are not 6)\n",
    "#print(\"IDs missing some entries (not appearing 6 times):\")\n",
    "#print(ids_missing)\n",
    "\n",
    "# Print the missing IDs and their occurrences specifically\n",
    "for id_part, count in ids_missing.items():\n",
    "    print(f\"ID: {id_part}, Present {count} times (Missing {6 - count} times)\")\n",
    "\n",
    "# Open a text file to write the output\n",
    "with open('id_counts/full_Data_idcounts_report.txt', 'w') as file:\n",
    "    file.write(f\"Row count: {len(df_clean_full)}\\n\")\n",
    "    for id_part, count in ids_missing.items():\n",
    "        file.write(f\"ID: {id_part}, Present {count} times (Missing {6 - count} times)\\n\")\n",
    "\n",
    "print(\"Output written to ids_report.txt\")\n",
    "\n",
    "# Full row data of the missing IDs,  can filter them out\n",
    "missing_rows = df_clean_full[df_clean_full['ID_Part'].isin(ids_missing.index)]\n",
    "print(\"Rows with IDs missing:\")\n",
    "print(missing_rows)\n",
    "\n",
    "# Define the specific ID_Part you're looking for\n",
    "specific_id_part = \"P7\"  # Replace with the specific ID part you want to print\n",
    "specific_id_part2 = \"P160\"\n",
    "\n",
    "# Filter the rows where ID_Part matches the specific ID\n",
    "matching_rows = df_clean_full[df_clean_full['ID_Part'] == specific_id_part]\n",
    "matching_rows2 = df_clean_full[df_clean_full['ID_Part'] == specific_id_part2]\n",
    "\n",
    "# Print the rows with the specific ID\n",
    "print(f\"Rows with ID part '{specific_id_part}':\")\n",
    "print(f\"Rows with ID part '{specific_id_part2}':\")\n",
    "\n",
    "print(matching_rows)\n",
    "print(matching_rows2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa57da4-76ff-420f-89ef-89fe88978ea4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Excel Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26ccc3-3fa8-4b41-bc5b-de1c391a2ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Log pictures2.csv'  \n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Print the initial number of rows to inspect\n",
    "print(f\"Initial number of rows: {len(df)}\")\n",
    "\n",
    "# Drop rows where all elements are NaN (likely to be empty rows)\n",
    "df_cleaned = df.dropna(how='all')\n",
    "\n",
    "# Find duplicates in the 'Log_ID' column\n",
    "duplicates = df_cleaned[df_cleaned.duplicated(subset='Log_ID', keep=False)]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate Rows Based on 'Log_ID' Column:\")\n",
    "print(duplicates)\n",
    "print(f\"Initial number of rows: {len(df_cleaned)}\")\n",
    "# Optionally, save the duplicates to a new CSV file\n",
    "#duplicates.to_csv('duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0e649-a26c-4493-812e-28529301e56f",
   "metadata": {},
   "source": [
    "There are three duplicate rows with different file names. \n",
    "* P17 in Site 16\n",
    "    * Actual image file names are 10-15, Remove row 78     \n",
    "* P216 in Site 16\n",
    "    * Actual image file names are 16-21. Remove row 131 \n",
    "* P78 in site 12\n",
    "    * Actual image file names are 850-855. Remove row 218"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3417d-35ff-4ce8-be0a-1e40f864e730",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Finding Missing Data\n",
    "\n",
    "Cross checking with excel log of the image data.\n",
    "\n",
    "First need to make log data comparable to SAM output data. Log data contains Log ID with image name as feature. For comparison there is a need for each row is image name with ID: **Log ID-image name**\n",
    "\n",
    "**log_ids.csv** will be used for crosschecking SAM generated data to find out missing IDs in generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5dbd3-5ee0-404d-92c6-51fb2652deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original data from CSV\n",
    "input_file = 'Log_pictures2.csv'\n",
    "df = pd.read_csv(input_file, sep=';')\n",
    "\n",
    "# Specify the row numbers (indices) to drop\n",
    "rows_to_drop = [77, 129, 215]\n",
    "df = df.drop(rows_to_drop)\n",
    "\n",
    "# Print the content of the rows to be dropped\n",
    "dropped_rows = df.iloc[rows_to_drop]\n",
    "print(\"Dropped rows:\")\n",
    "print(dropped_rows)\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "print(df)\n",
    "# Step 2: Create an empty list to hold the new rows\n",
    "new_data = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    log_id = row['Log_ID']\n",
    "    \n",
    "    # Loop through each feature\n",
    "    for feature in ['Photo_1-2', 'Photo_2-3', 'Photo_3-4', 'Photo_4-1', 'Photo_top', 'Photo_bottom']:\n",
    "        # Create a new Log_ID-data entry\n",
    "        new_entry = {\n",
    "            'Log_ID': f\"{log_id}-{row[feature]}\"\n",
    "        }\n",
    "        # Append this new entry to the new_data list\n",
    "        new_data.append(new_entry)\n",
    "\n",
    "# Create a new DataFrame from the new_data list\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Export the new DataFrame to CSV\n",
    "output_file = 'log_ids.csv'\n",
    "new_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'New CSV file saved as: {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db169c94-027b-415e-b84c-172bcc902cbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Export Missing IDs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ae704-e36a-4237-a04f-266eeb89f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read all three CSV files\n",
    "original_file = 'log_ids.csv'\n",
    "#comparison_file_1 = 'cleaned_data/pine_merged_dataset.csv'\n",
    "#comparison_file_2 = 'cleaned_data/spruce_merged_dataset.csv'\n",
    "comparison_file = 'cleaned_data/sorted_full_dataset.csv'\n",
    "\n",
    "original_df = pd.read_csv(original_file, sep=';')\n",
    "#comparison_df_1 = pd.read_csv(comparison_file_1)\n",
    "#comparison_df_2 = pd.read_csv(comparison_file_2)\n",
    "comparison_df = pd.read_csv(comparison_file)\n",
    "\n",
    "# Step 2: Extract the unique Log_ID columns from all three files\n",
    "# use an outer join to make sure we keep all Log_IDs from both files\n",
    "#merged_comparison_df = pd.merge(comparison_df_1, comparison_df_2, on='ID', how='outer')\n",
    "\n",
    "# Step 3: Extract the unique Log_IDs from both the original file and the merged comparison file\n",
    "original_log_ids = original_df['Log_ID'].unique()  # Unique Log_IDs from original\n",
    "#merged_comparison_log_ids = merged_comparison_df['ID'].unique()  # Unique Log_IDs from merged comparison file\n",
    "comparison_log_ids = comparison_df['ID'].unique()\n",
    "\n",
    "# Step 4: Find the missing Log_IDs in the merged comparison file compared to the original file\n",
    "#missing_in_merged_comparison = set(original_log_ids) - set(merged_comparison_log_ids)\n",
    "missing_in_merged_comparison = set(original_log_ids) - set(comparison_log_ids)\n",
    "\n",
    "# Step 5: Sort the missing Log_IDs\n",
    "sorted_missing_log_ids = sorted(missing_in_merged_comparison)\n",
    "print(len(sorted_missing_log_ids))\n",
    "\n",
    "# Step 6: Print and save the missing Log_IDs to a text file\n",
    "output_text_file = 'missing_log_ids_fulldata.txt'\n",
    "\n",
    "with open(output_text_file, 'w') as file:\n",
    "    file.write(\"Missing Log_IDs in the merged comparison file:\\n\")\n",
    "    for log_id in sorted_missing_log_ids:\n",
    "        file.write(f\"{log_id}\\n\")\n",
    "\n",
    "print(f\"Missing Log_IDs have been saved to: {output_text_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f760d-49fe-411a-9629-47d2e7409351",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Find Missing Files Folderpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139449b-0450-4ad4-9c98-3a562c9e3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Step 1: Read the missing Log_IDs from the text file\n",
    "missing_log_ids_file = 'missing_log_ids.txt'\n",
    "\n",
    "with open(missing_log_ids_file, 'r') as file:\n",
    "    missing_log_ids = file.readlines()\n",
    "\n",
    "# Clean up the list and remove the header\n",
    "missing_log_ids = [log_id.strip() for log_id in missing_log_ids if log_id.strip()]\n",
    "\n",
    "# Step 2: Define the two main folders and the subfolder name\n",
    "main_folder_1 = 'pine_sites'\n",
    "main_folder_2 = 'spruce_sites'\n",
    "original_folder_name = 'original'\n",
    "\n",
    "# Step 3: Function to find files containing Log_IDs in the original folder\n",
    "def find_log_id_files(log_id, folder_path):\n",
    "    matching_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if log_id in file:  # Check if Log_ID is in the file name\n",
    "                matching_files.append(os.path.join(root, file))\n",
    "    return matching_files\n",
    "\n",
    "# Step 4: Search for missing Log_IDs in all 'original' folders of both main folders\n",
    "results = {}\n",
    "\n",
    "# Iterate over each of the 30 subfolders for both main folders\n",
    "for main_folder in [main_folder_1, main_folder_2]:\n",
    "    for i in range(1, 31):  # Folder names are \"Folder 1\", \"Folder 2\", ..., \"Folder 30\"\n",
    "        folder_path = os.path.join(main_folder, f\"Site {i}\", original_folder_name)\n",
    "\n",
    "        # Search for each missing Log_ID in the current original folder\n",
    "        for log_id in missing_log_ids:\n",
    "            if log_id not in results:\n",
    "                results[log_id] = []  # Initialize if not already done\n",
    "            \n",
    "            matching_files = find_log_id_files(log_id, folder_path)\n",
    "            results[log_id].extend(matching_files)\n",
    "            \n",
    "# Step 5: Print or save the results\n",
    "output_results_file = 'log_id_locations.txt'\n",
    "\n",
    "with open(output_results_file, 'w') as file:\n",
    "    for log_id, file_paths in results.items():\n",
    "        file.write(f\"Log_ID: {log_id}\\n\")\n",
    "        if file_paths:\n",
    "            for path in file_paths:\n",
    "                file.write(f\"  {path}\\n\")\n",
    "        else:\n",
    "            file.write(\"  None\\n\")\n",
    "        \n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Locations of missing Log_IDs have been saved to: {output_results_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c41e4-d590-4a49-ae00-82d140dd53bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace File Extension \"JPG\" with \"png\" in Data Set\n",
    "\n",
    "Target exported images have transparency with \"png\" extension but stored data is JPG extension. Same data but name change is necessary for cleaning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923844e1-2a2e-4c4b-ad0e-19beed3cab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "original_csv_path = 'mask_areas_missing.csv'\n",
    "df = pd.read_csv(original_csv_path)\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original one\n",
    "new_df = df.copy()\n",
    "\n",
    "# Modify the 'ID' column, replacing 'cropped' with 'transparent' and 'JPG' with 'png'\n",
    "new_df['ID'] = new_df['ID'].str.replace('JPG', 'png')\n",
    "\n",
    "# Define the path to save the new CSV file\n",
    "new_csv_path = 'mask_areas_missing_IDmod.csv'\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "new_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(\"New dataset has been created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def150c-9347-4789-a38f-3656d7700ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rename Exported Files\n",
    "\n",
    "Exported files contain \"transparent\" tag. Export algorithm saves data with \"cropped\" tag. Renaming image files is necessary to keep the needed data from output log. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad7ba8-da7e-4a96-8e80-8ec4b2d8c2ab",
   "metadata": {},
   "source": [
    "#### For \"Missing Images\" File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b10de5-8062-4ec2-ab30-011aff9b8f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the main directory containing the 'cropped' folder\n",
    "main_folder = 'Reruns/cropped'\n",
    "\n",
    "# Function to rename files in each site folder inside 'pine' and 'spruce'\n",
    "def rename_files_in_folder(folder_path):\n",
    "    # Iterate through each file in the site folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if 'transparent' is in the filename\n",
    "        if 'transparent' in filename:\n",
    "            # Create the new filename by replacing 'transparent' with 'cropped'\n",
    "            new_filename = filename.replace('transparent', 'cropped')\n",
    "            \n",
    "            # Get the full paths for the old and new filenames\n",
    "            old_file_path = os.path.join(folder_path, filename)\n",
    "            new_file_path = os.path.join(folder_path, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed: {filename} to {new_filename}\")\n",
    "\n",
    "# Iterate through subfolders ('pine', 'spruce') in the 'cropped' folder\n",
    "for tree_type in os.listdir(main_folder):\n",
    "    tree_type_path = os.path.join(main_folder, tree_type)\n",
    "    \n",
    "    # Check if it's a directory (e.g., 'pine', 'spruce')\n",
    "    if os.path.isdir(tree_type_path):\n",
    "        # Iterate through site folders within each tree type (pine, spruce)\n",
    "        for site_folder in os.listdir(tree_type_path):\n",
    "            site_folder_path = os.path.join(tree_type_path, site_folder)\n",
    "            \n",
    "            # Check if it's a directory (site folder)\n",
    "            if os.path.isdir(site_folder_path):\n",
    "                rename_files_in_folder(site_folder_path)\n",
    "\n",
    "print(\"File renaming completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9007e-d8d2-4e1b-b7ce-f84d7d696de5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove Tags from ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd470d-2cab-4930-a12a-3d6546037888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the ID column. Remove unnecessary indicators that came from SAM output.\n",
    "\n",
    "# Load CSV from previous step to clean ID column's rows.\n",
    "csv_path = 'extras_andmissing_data.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Remove characters after '_' in the ID column\n",
    "df['ID'] = df['ID'].str.split('_').str[0]\n",
    "\n",
    "# Save the modified dataframe to new CSV\n",
    "output_csv_path = 'cleaned_data/extras_andmissing_ID_cleaned.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Modified CSV has been saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072c773-e458-49f0-bb11-10f67cbb8019",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clean Output Data from Unnecessary Mask Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b92ca-cf9a-4490-a394-1ed6662ce9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths\n",
    "#main_folder1_path = 'Reruns/cropped/pine/'\n",
    "#main_folder2_path = 'Reruns/cropped/spruce/'\n",
    "main_folder2_path = 'Reruns/cropped/extras/'\n",
    "\n",
    "# Define the output CSV file paths\n",
    "#output_csv_set1 = 'pine_missing_data.csv'\n",
    "output_csv_set2 = 'extras_andmissing_data.csv'\n",
    "\n",
    "# Function to extract filenames from all subfolders of a given folder\n",
    "def get_image_filenames_from_folder(folder_path):\n",
    "    image_filenames = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png')):\n",
    "                image_filenames.append(file)  # append just the filename (without the path)\n",
    "    return image_filenames\n",
    "\n",
    "# Function to filter data based on filenames\n",
    "def filter_data_by_filenames(data, filenames):\n",
    "    # Data has a column ID' that stored the image filename\n",
    "    filtered_data = data[data['ID'].isin(filenames)]\n",
    "    return filtered_data\n",
    "\n",
    "# Load your original CSV dataset\n",
    "original_data = pd.read_csv('mask_areas_extras_missingIDmod.csv')\n",
    "\n",
    "# Get image filenames from both main folders and their subfolders\n",
    "#filenames_set1 = get_image_filenames_from_folder(main_folder1_path)\n",
    "filenames_set2 = get_image_filenames_from_folder(main_folder2_path)\n",
    "\n",
    "# Filter the original data for Set 1 and Set 2\n",
    "#set1_data = filter_data_by_filenames(original_data, filenames_set1)\n",
    "set2_data = filter_data_by_filenames(original_data, filenames_set2)\n",
    "\n",
    "# Save the filtered data into two CSV files\n",
    "#set1_data.to_csv(output_csv_set1, index=False)\n",
    "set2_data.to_csv(output_csv_set2, index=False)\n",
    "\n",
    "print(\"Data saved to CSV files successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a2923-407c-48aa-af31-5ca54a70f3c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File Name Cleaning\n",
    "\n",
    "Remove tags that came from SAM algorithm. \"cropped\", \"transparent\" and separator \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39765f-a442-4956-bbe6-e63c90f4ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the path to the main folder\n",
    "main_folder = 'D:\\internship_images\\extras'\n",
    "\n",
    "# Iterate through the subfolders\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check for 'Target folder'\n",
    "        for target_folder in ['background', 'cropped']:\n",
    "            target_folder_path = os.path.join(subfolder_path, target_folder)\n",
    "            \n",
    "            # Ensure the target folder exists\n",
    "            if os.path.exists(target_folder_path):\n",
    "                # Iterate through the files in the target folder\n",
    "                for filename in os.listdir(target_folder_path):\n",
    "                    file_path = os.path.join(target_folder_path, filename)\n",
    "                    \n",
    "                    # If it's a file, process it\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Remove everything after the first underscore (including it)\n",
    "                        new_filename = re.sub(r'_.*', '', filename)\n",
    "                        \n",
    "                        # Check if the new filename already ends with .png\n",
    "                        if not new_filename.endswith('.png'):\n",
    "                            new_filename += '.png'\n",
    "                        \n",
    "                        # Form the new file path\n",
    "                        new_file_path = os.path.join(target_folder_path, new_filename)\n",
    "                        \n",
    "                        # Rename the file only if the name changes\n",
    "                        if file_path != new_file_path:\n",
    "                            os.rename(file_path, new_file_path)\n",
    "                            print(f'Renamed: {file_path} -> {new_file_path}')\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d91e1ec-1642-4e6e-bd37-1c87cf212528",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove Excess .png\n",
    "There were extra .pngs added from previous code. Code is fixed but this code will remove the excess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e031ec-57a3-441b-8f63-268a4e0e1729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the path to the main folder\n",
    "main_folder = 'normalization_test'\n",
    "\n",
    "# Iterate through the subfolders\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Check for 'Target folder'\n",
    "        for target_folder in ['background', 'cropped', 'bottom', 'top-bottom']:\n",
    "            target_folder_path = os.path.join(subfolder_path, target_folder)\n",
    "            \n",
    "            # Ensure the target folder exists\n",
    "            if os.path.exists(target_folder_path):\n",
    "                # Iterate through the files in the target folder\n",
    "                for filename in os.listdir(target_folder_path):\n",
    "                    file_path = os.path.join(target_folder_path, filename)\n",
    "                    \n",
    "                    # If it's a file, process it\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Remove extra .png extensions\n",
    "                        new_filename = re.sub(r'(\\.png)+$', '.png', filename)\n",
    "                        \n",
    "                        # Form the new file path\n",
    "                        new_file_path = os.path.join(target_folder_path, new_filename)\n",
    "                        \n",
    "                        # Rename the file only if the name changes\n",
    "                        if file_path != new_file_path:\n",
    "                            os.rename(file_path, new_file_path)\n",
    "                            print(f'Renamed: {file_path} -> {new_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f04d1-2474-4c99-bfe4-bad49fc772a4",
   "metadata": {},
   "source": [
    "## Testing Duplicates with Overlay and Pixel Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70cfe7-472d-4150-8ccc-8927e6a4bdd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Overlay Cropped Image to Original Image\n",
    "\n",
    "This allows testing for using coordinates extracted during mask creation to be tested on original image. There are multiple duplicates in the data set. Testing by hand with overlaying cropped mask onto original image is safest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7474812-405b-483d-be35-dac0b4d0ddf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ExifTags\n",
    "\n",
    "# Paths to the two PNG images\n",
    "base_image_path = 'D:\\Internship\\spruce_sites/Site 1/original/S7-739.JPG'  # Base image (background)\n",
    "overlay_image_path = ('D:\\Internship\\spruce_sites/Site 1/cropped/S7-739_transparent_3.png')  # Image to overlay\n",
    "\n",
    "# Open the base and overlay images\n",
    "base_image = Image.open(base_image_path).convert(\"RGBA\")\n",
    "overlay_image = Image.open(overlay_image_path).convert(\"RGBA\")\n",
    "\n",
    "# Known top-left coordinates for overlay_image on base_image\n",
    "overlay_top_left_x = 350  # Example X coordinate\n",
    "overlay_top_left_y = 1345 # Example Y coordinate\n",
    "\n",
    "# Correct the orientation of the base image explicitly\n",
    "base_image = base_image.rotate(-90, expand=True)  # Rotate by 0 degrees to ensure no rotation\n",
    "\n",
    "# Strip EXIF data from both images (especially from base_image to prevent rotation)\n",
    "#base_image = strip_exif(base_image)\n",
    "#overlay_image = strip_exif(overlay_image)\n",
    "\n",
    "# Create a blank image with the same size as the base image to handle transparency\n",
    "# This is necessary because `paste()` does not blend transparency; it replaces pixels.\n",
    "temp_image = Image.new(\"RGBA\", base_image.size)\n",
    "\n",
    "# Paste the base image onto the temporary image\n",
    "temp_image.paste(base_image, (0, 0))\n",
    "\n",
    "# Paste the overlay image onto the temporary image at the given (x, y) coordinates\n",
    "temp_image.paste(overlay_image, (overlay_top_left_x, overlay_top_left_y), overlay_image)\n",
    "\n",
    "# Save the final combined image\n",
    "temp_image.save('combined_image.png')\n",
    "\n",
    "# Optionally, show the result\n",
    "temp_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53226b5b-b932-43fa-9aab-ae4f4c4fc4e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Calculate Area of PNG Image\n",
    "\n",
    "This section helps with calculating the area of the cropped mask. In some cases, while testing coodinates to eliminate duplicates, finding out the area of the image makes it faster to eliminate duplicate rows.\n",
    "\n",
    "Images are PNG and **alpha channel** is zero (transparent) in areas that are not interested. Below code counts pixels that are not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f91f2-1168-4e8c-b0c7-f2237f7de317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open the image file (make sure it has an alpha channel)\n",
    "image = Image.open('D:\\Internship\\spruce_sites/Site 1/cropped/S7-739_transparent_3.png').convert(\"RGBA\")\n",
    "\n",
    "# Get the pixel data\n",
    "pixels = image.getdata()\n",
    "\n",
    "# Initialize a counter for non-transparent pixels\n",
    "non_transparent_pixel_count = 0\n",
    "\n",
    "# Loop through pixels and count those with non-zero alpha (i.e., not fully transparent)\n",
    "for pixel in pixels:\n",
    "    # pixel[3] is the alpha channel value in RGBA mode\n",
    "    if pixel[3] > 0:  # Adjust the threshold if you want to consider partial transparency\n",
    "        non_transparent_pixel_count += 1\n",
    "\n",
    "# Output the non-transparent area (in pixels)\n",
    "print(f\"Non-transparent area: {non_transparent_pixel_count} pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53100ea-7eb1-4886-b2ec-1bf6ca0eb8dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CSV to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856390a-3eb7-4b85-9051-ff7b4e07970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to CSV file\n",
    "csv_file = 'cleaned_data/main_data_corrected_areas_no_proportion.csv'\n",
    "\n",
    "# Specify the path to save the filtered Excel file\n",
    "converted_excel_file = 'cleaned_data/main_data_corrected_areas_no_proportion.xlsx'\n",
    "\n",
    "# List of values to filter\n",
    "#values_to_filter = ['P15-108', 'P30-640', 'P31-877', 'P47-115', 'P47-116', 'P49-514', 'P49-515', 'P49-517', 'P76-294', 'P86-122', 'P92-858',\n",
    "#                    'P96-889', 'P123-287', 'P153-869', 'P156-882', 'P156-884', 'P157-127', 'P188-146', 'P206-151', 'P206-152', 'P229-522',\n",
    "#                   'P229-523']  # Replace with desired IDs\n",
    "\n",
    "# Column to filter by\n",
    "#column_to_filter = 'file_name'  # Replace with the name of the column to filter\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Filter rows where the column matches any of the IDs in the list\n",
    "#filtered_data = data[data[column_to_filter].isin(values_to_filter)]\n",
    "\n",
    "# Write the filtered data to an Excel file\n",
    "data.to_excel(converted_excel_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Filtered rows saved to {converted_excel_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd0269-41da-4a20-b927-8160a34fef9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modify Area Data Set to new structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81ef10-eac8-48dd-af9c-94c923f7ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to CSV file\n",
    "csv_file = 'cleaned_data/sorted_full_dataset.csv'\n",
    "\n",
    "# Specify the path to save the modified CSV file\n",
    "modified_csv_file = 'cleaned_data/sorted_full_dataset_mod.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Rename the 'ID' column to 'file_name'\n",
    "data.rename(columns={'ID': 'file_name'}, inplace=True)\n",
    "\n",
    "# Create a new 'ID' column by extracting the part before the \"-\" in the 'file_name' column\n",
    "data['ID'] = data['file_name'].str.split('-').str[0]\n",
    "\n",
    "# Rearrange the columns in the specified order\n",
    "column_order = ['ID', 'file_name', 'top_left_x', 'top_left_y', 'area']\n",
    "data = data[column_order]\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv(modified_csv_file, index=False)\n",
    "\n",
    "print(f\"Modified dataset saved to {modified_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45559834-2eab-46aa-a890-2f032d2c0ae8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Merge Area Data and Lichen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39b0ab-e8e6-4588-beb1-8a69b0a99396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to the primary and secondary CSV files\n",
    "primary_csv = 'cleaned_data/full_data_lichen.csv'\n",
    "secondary_csv = 'cleaned_data/lichen_area_results_extras_test.csv'\n",
    "\n",
    "# Load the primary and secondary CSV files\n",
    "primary_data = pd.read_csv(primary_csv)\n",
    "secondary_data = pd.read_csv(secondary_csv)\n",
    "\n",
    "# Merge the DataFrames on the 'file_name' column\n",
    "merged_data = pd.merge(primary_data, secondary_data, on='file_name', how='left')\n",
    "\n",
    "# If both DataFrames have an 'ID' column, resolve the conflict\n",
    "if 'ID_x' in merged_data.columns and 'ID_y' in merged_data.columns:\n",
    "    # Keep only one ID column, for example, ID_x (primary dataset's ID)\n",
    "    merged_data['ID'] = merged_data['ID_x']\n",
    "    merged_data.drop(['ID_x', 'ID_y'], axis=1, inplace=True)\n",
    "\n",
    "# Rearrange the columns to place 'ID' first\n",
    "columns_order = ['ID'] + [col for col in merged_data.columns if col != 'ID']\n",
    "merged_data = merged_data[columns_order]\n",
    "\n",
    "# Save the cleaned merged dataset\n",
    "merged_csv = 'cleaned_data/full_data_lichen_extras.csv'\n",
    "merged_data.to_csv(merged_csv, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to {merged_csv}\")\n",
    "\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc16386-eb62-4ab1-b0d7-963878bd75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load primary and secondary datasets\n",
    "primary_csv = 'cleaned_data/full_data_lichen.csv'\n",
    "secondary_csv = 'cleaned_data/lichen_area_results_extras_test.csv'\n",
    "\n",
    "primary_data = pd.read_csv(primary_csv)\n",
    "secondary_data = pd.read_csv(secondary_csv)\n",
    "\n",
    "# Merge secondary data into primary data using 'ID' and 'file_name'\n",
    "merged_data = primary_data.merge(secondary_data, on=['ID', 'file_name'], how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Fill missing values in primary columns with values from secondary dataset\n",
    "for col in ['green_pixels', 'white_pixels', 'overlap_pixels', 'green_exclusive', 'white_exclusive', 'side_lichen_area']:\n",
    "    if col + '_new' in merged_data.columns:  \n",
    "        merged_data[col] = merged_data[col].fillna(merged_data[col + '_new'])\n",
    "        merged_data.drop(columns=[col + '_new'], inplace=True)  # Remove unnecessary '_new' column\n",
    "\n",
    "# Save the final merged dataset\n",
    "merged_csv = 'cleaned_data/full_data_lichen_extras.csv'\n",
    "merged_data.to_csv(merged_csv, index=False)\n",
    "\n",
    "print(f\"Merged dataset saved to {merged_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d78f89-bd6b-484c-9675-455281a35eab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Update Missing data rows in Main Data before Feature Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37bed2-e11f-4f33-8c2a-507936dcf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your files and matching column\n",
    "main_file = \"cleaned_data/main_data_corrected_areas_no_proportion.csv\"   # main dataset to update\n",
    "new_file = \"S168_lichen.csv\"    # dataset with new/updated data\n",
    "match_col = \"file_name\"       # change this to match column name\n",
    "\n",
    "# Load datasets\n",
    "df_main = pd.read_csv(main_file)\n",
    "df_new = pd.read_csv(new_file)\n",
    "\n",
    "# SStandardize the match column\n",
    "df_main[match_col] = df_main[match_col].astype(str).str.strip()\n",
    "df_new[match_col] = df_new[match_col].astype(str).str.strip()\n",
    "\n",
    "# Find common columns (excluding the match column)\n",
    "common_cols = [col for col in df_main.columns if col in df_new.columns and col != match_col]\n",
    "print(f\"Common columns to update: {common_cols}\")\n",
    "\n",
    "# Set the match column as index for easy alignment \n",
    "df_main.set_index(match_col, inplace=True)\n",
    "df_new.set_index(match_col, inplace=True)\n",
    "\n",
    "# Step 6: Identify which rows will be updated ===\n",
    "rows_to_update = df_main.index.intersection(df_new.index)\n",
    "print(f\"\\n Number of matching rows to update: {len(rows_to_update)}\")\n",
    "\n",
    "# Optional: Preview which rows will be updated\n",
    "display(df_main.loc[rows_to_update].head(5))\n",
    "\n",
    "# Update only shared columns \n",
    "df_main.update(df_new[common_cols])\n",
    "\n",
    "# Reset index and preview result \n",
    "df_main.reset_index(inplace=True)\n",
    "\n",
    "print(\"\\n Dataset updated successfully. Preview of updated dataset:\")\n",
    "display(df_main.head(10))\n",
    "\n",
    "# (Optional) Save updated dataset \n",
    "df_main.to_csv(\"updated_main_dataset_S168.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8de722-b1b9-4b43-8d31-f657de970c75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Calculation For Final Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775246f-34bf-405b-beea-a64124e53795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"updated_main_dataset_S168.csv\")\n",
    "\n",
    "\n",
    "id_col = \"ID\"  \n",
    "area_col = \"area\"  \n",
    "lichen_cols = [\"green_exclusive\", \"white_exclusive\", \"overlap_pixels\"]  \n",
    "# Create a new column for total lichen area per row\n",
    "df[\"lichen_area_total\"] = df[lichen_cols].sum(axis=1)\n",
    "\n",
    "# Drop rows where all 3 lichen areas are missing\n",
    "df_clean = df.dropna(subset=lichen_cols, how=\"all\").copy()\n",
    "\n",
    "# --- Group by ID and calculate ---\n",
    "summary = (\n",
    "    df_clean\n",
    "    .groupby(id_col)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"total_area\": g[area_col].sum(),\n",
    "        \"mean_area\": g[area_col].sum() / 4,  # always 4 rows per ID\n",
    "        \"total_lichen_area\": g[\"lichen_area_total\"].sum(),\n",
    "        \"mean_lichen_area\": g[\"lichen_area_total\"].sum() / 4,\n",
    "        \"mean_lichen_prop\": round((g[\"lichen_area_total\"].sum() / 4) / (g[area_col].sum() / 4), 3),\n",
    "        \"mean_lichen_percent\": round((g[\"lichen_area_total\"].sum() / 4) / (g[area_col].sum() / 4) * 100, 3)\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Export cleaned dataset and summary ---\n",
    "df_clean.to_csv(\"main_lichen_dataset2.csv\", index=False)\n",
    "df_clean.to_excel(\"main_lichen_dataset2.xlsx\", index=False)\n",
    "\n",
    "summary.to_csv(\"final_main_simplified_dataset2.csv\", index=False)\n",
    "summary.to_excel(\"final_main_simplified_dataset2.xlsx\", index=False)\n",
    "\n",
    "print(\" Cleaned dataset (with per-row lichen totals) and grouped summary exported as CSV & Excel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc43d3-a1ed-485c-bbac-315182e87673",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Add \"proportion\", percent to data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8c278-9c52-4201-8c7a-0ee10204aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Path to your CSV file\n",
    "csv_file = 'cleaned_data/main_data_corrected_areas_no_proportion.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Calculate the proportion: pixel_area / area\n",
    "data['lichen_proportion'] = data['side_lichen_area'] / data['area']\n",
    "\n",
    "# Lichen percent\n",
    "data['lichen_percent'] = data['lichen_proportion'] * 100\n",
    "\n",
    "# Lichen percent rounded\n",
    "data['lichen_percent_3_decimal'] = (data['lichen_percent']).round(3)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "modified_csv_file = 'cleaned_data/main_data_corrected_areas_prop.csv'\n",
    "data.to_csv(modified_csv_file, index=False)\n",
    "\n",
    "print(f\"Modified dataset with proportions saved to {modified_csv_file}\")\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5851bcf-0a5d-455f-9bc3-c34c00117682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "csv_file = 'cleaned_data/main_data_corrected_areas_prop.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Calculate mean percentage for each ID, ignoring NaN values\n",
    "mean_percentages = data.groupby('ID')['lichen_percent'].mean().reset_index()\n",
    "\n",
    "# Rename the new column\n",
    "mean_percentages.rename(columns={'lichen_percent': 'mean_percent'}, inplace=True)\n",
    "\n",
    "# Merge the mean percentage back into the original dataset as a new column\n",
    "data = data.merge(mean_percentages, on='ID', how='left')\n",
    "\n",
    "# Lichen mean percent rounded\n",
    "data['lichen_mean_percent_3_decimal'] = (data['mean_percent']).round(3)\n",
    "\n",
    "# Save the updated dataset as a new file\n",
    "output_csv = 'cleaned_data/main_data_corrected_areas_prop_3dec.csv'\n",
    "data.to_csv(output_csv, index=False)\n",
    "print(data.head(40))\n",
    "print(f\"New dataset with 'mean_percent' column saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36431b-809b-4fc7-839b-8fce123d18d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Update Data set with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f448bc-af5c-4e05-9364-f9192912a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the primary and secondary datasets\n",
    "primary_df = pd.read_csv('cleaned_data/full_data_lichen_prop_extras.csv')  \n",
    "secondary_df = pd.read_csv('lichen_area_results_spruce_extras.csv')\n",
    "\n",
    "# Ensure the ID column is the same in both datasets\n",
    "# For example, if the ID column is named 'ID':\n",
    "primary_df.set_index('file_name', inplace=True)\n",
    "secondary_df.set_index('file_name', inplace=True)\n",
    "\n",
    "# Update the primary dataset with values from the secondary dataset\n",
    "# This will overwrite values in the primary dataset with non-null values from the secondary dataset\n",
    "primary_df.update(secondary_df)\n",
    "\n",
    "# Reset the index if needed\n",
    "primary_df.reset_index(inplace=True)\n",
    "\n",
    "# Save the updated primary dataset (optional)\n",
    "primary_df.to_csv('updated_lichen_area_dataset.csv', index=False)\n",
    "updated = pd.read_csv('updated_lichen_area_dataset.csv')\n",
    "print(updated.head(10))\n",
    "print(updated.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c3c8f-5396-4887-9c34-d64eb80ed32f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bark Thickness Data Sort and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bc0bd-8f45-4bb8-bec4-449e6ee5db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV file\n",
    "file_path = \"bark_thickness_main/thickness_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data points {len(df)}\")\n",
    "# Define the column to process\n",
    "column_name = \"thickness\"  # \"thickness\" column\n",
    "\n",
    "# Sort the column in ascending order\n",
    "df_sorted = df.sort_values(by=column_name)\n",
    "\n",
    "# Export the sorted data to a new CSV file\n",
    "sorted_file_path = \"bark_thickness_main/sorted_thickness_data.csv\"\n",
    "df_sorted.to_csv(sorted_file_path, index=False)\n",
    "print(f\"Sorted data saved as {sorted_file_path}\")\n",
    "\n",
    "# Count zeros in the column\n",
    "zero_count = (df_sorted[column_name] == 0).sum()\n",
    "print(f\"Number of zeros in '{column_name}':\", zero_count)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_sorted[column_name], bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel(column_name)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Histogram of {column_name}\")\n",
    "\n",
    "# Export histogram as an image\n",
    "histogram_path = \"bark_thickness_main/data_histogram.png\"\n",
    "plt.savefig(histogram_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Histogram saved as {histogram_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55c45a-d499-4a92-906b-bc82ac452be5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exploring Lichen Data for Abnormal Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f483b0-8170-4271-a00a-a9e7b8f974db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV file\n",
    "file_path = \"cleaned_data/updated_lichen_area_dataset_prop_3dec.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the two columns to analyze\n",
    "column1 = \"green_pixels\"  \n",
    "column2 = \"white_pixels\" \n",
    "\n",
    "output_folder = \"lichen_data_exploration\" \n",
    "\n",
    "# Function to analyze a column\n",
    "def analyze_column(column_name):\n",
    "    print(f\"\\n### Analysis for '{column_name}' ###\\n\")\n",
    "    \n",
    "    # Compute IQR\n",
    "    Q1 = df[column_name].quantile(0.25)  \n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Outlier detection thresholds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find outliers\n",
    "    outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "    print(f\"Number of Outliers in '{column_name}': {len(outliers)}\")\n",
    "    print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "    # Get top 15 highest values\n",
    "    top_15 = df.nlargest(15, column_name)\n",
    "    print(\"\\nTop 15 Highest Values:\\n\", top_15)\n",
    "\n",
    "    # Get 15 lowest values\n",
    "    bottom_15 = df.nsmallest(15, column_name)\n",
    "    print(\"\\n15 Lowest Values:\\n\", bottom_15)\n",
    "\n",
    "    # Ensure the folder exists (create if not)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save the CSV files in the specified folder\n",
    "    outliers.to_csv(os.path.join(output_folder, f\"outliers_{column_name}.csv\"), index=False)\n",
    "    top_15.to_csv(os.path.join(output_folder, f\"top_15_{column_name}.csv\"), index=False)\n",
    "    bottom_15.to_csv(os.path.join(output_folder, f\"bottom_15_{column_name}.csv\"), index=False)\n",
    "    \n",
    "    print(f\"Files saved in: {output_folder}\")\n",
    "\n",
    "    print(f\"\\nResults saved: 'outliers_{column_name}.csv', 'top_15_{column_name}.csv', 'bottom_15_{column_name}.csv'\\n\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Analyze both columns\n",
    "analyze_column(column1)\n",
    "analyze_column(column2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32d458-f59e-42c4-b1e0-c9653688af77",
   "metadata": {},
   "source": [
    "### Find the Outlier Image Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4363322-80e6-48f4-b5ad-c3e0645562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the main folder path\n",
    "main_folder = \"D:\\internship_images\"  # Main folder\n",
    "target_folder = \"lichens\"  # Change with the specific folder name to search in\n",
    "\n",
    "# Load the datasets containing spotted data (outliers, top/bottom 15)\n",
    "outliers_file = \"lichen_data_exploration/outliers_white_pixels.csv\"  # Update with actual file name\n",
    "top_15_file = \"lichen_data_exploration/top_15_white_pixels.csv\"\n",
    "bottom_15_file = \"lichen_data_exploration/bottom_15_white_pixels.csv\"\n",
    "\n",
    "# Read the CSV files\n",
    "outliers_df = pd.read_csv(outliers_file)\n",
    "top_15_df = pd.read_csv(top_15_file)\n",
    "bottom_15_df = pd.read_csv(bottom_15_file)\n",
    "\n",
    "# Extract file names from all datasets\n",
    "file_names = set(outliers_df[\"file_name\"]).union(top_15_df[\"file_name\"], bottom_15_df[\"file_name\"])\n",
    "\n",
    "# Function to search for images only in the specified target folder\n",
    "def find_image_paths(main_folder, target_folder, file_names):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        if os.path.basename(root) == target_folder:  # Only search inside target folder\n",
    "            for file in files:\n",
    "                if file.endswith(\".jpg\") and file[:-4] in file_names:  # Match file_name with \".png\"\n",
    "                    image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "# Find image paths in the target folder\n",
    "image_paths = find_image_paths(main_folder, target_folder, file_names)\n",
    "\n",
    "# Save image paths to a text file\n",
    "output_txt = \"lichen_data_exploration/white_spotted_image_paths.txt\"\n",
    "with open(output_txt, \"w\") as f:\n",
    "    for path in image_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} images in '{target_folder}'. Paths saved in '{output_txt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac83a35-3e0f-4010-9fdc-38e8b0a66e5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Thickness estimation methods comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e803fc-c937-4d8c-85ed-2f3d13411764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"bark_thickness/contrast_test_extended_min_S.csv\")\n",
    "\n",
    "# Apply dark background style\n",
    "#plt.style.use(\"dark_background\")\n",
    "\n",
    "# Create base file name column (removing \"_c\" for modified entries)\n",
    "df[\"base_name\"] = df[\"file_name\"].str.replace(\"_c\", \"\", regex=True)\n",
    "\n",
    "# Separate original and modified data\n",
    "df_original = df[~df[\"file_name\"].str.contains(\"_c\")]\n",
    "df_modified = df[df[\"file_name\"].str.contains(\"_c\")]\n",
    "\n",
    "# Plot original data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df_original[\"file_name\"], df_original[\"thickness\"], label=\"Original\", color=\"red\", marker=\"v\", alpha=0.7)\n",
    "\n",
    "# Plot modified data using base file name for X-axis\n",
    "plt.scatter(df_modified[\"base_name\"], df_modified[\"thickness\"], label=\"Modified\", color=\"black\", marker=\"^\", alpha=0.7)\n",
    "\n",
    "# Customization\n",
    "plt.xlabel(\"File Name (File Name for Contrast Modified Images )\")\n",
    "plt.ylabel(\"Thickness\", color=\"black\")\n",
    "plt.title(\"Comparison of Original and Contrast Modified Images using Saturation component\", color=\"black\")\n",
    "plt.xticks(rotation=90, color=\"black\")  # Rotate labels for better visibility\n",
    "plt.yticks(color=\"black\")\n",
    "plt.legend(title=\"Category\")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"bark_thickness/contrast_test_min_S.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd913f-5677-41ea-9e40-295eb78fcde9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plots for Bark thickness Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b8bd6-cb36-4d12-b56f-f58834ffd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2hsv\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "def compute_bark_thickness(image_path, save_dir):\n",
    "    \"\"\"Computes bark thickness and saves multiple plots including overlay on transformed image.\"\"\"\n",
    "    image = imread(image_path)\n",
    "    rgb_image = image[..., :3]\n",
    "    mask = (image[..., 3] > 0).astype(np.uint8) * 255  # Alpha channel as mask\n",
    "    \n",
    "    hsv_image = rgb2hsv(rgb_image)\n",
    "    \"\"\"Select appropriate component to be used in computation Saturation or Value.\"\"\"\n",
    "    #component = hsv_image[..., 1] # Saturation\n",
    "    component = hsv_image[..., 2] # Value\n",
    "\n",
    "    value_matrix = -1 * np.ones((3601, 629))\n",
    "    center_x, center_y = image.shape[0] / 2, image.shape[1] / 2\n",
    "    \n",
    "    idx = 0\n",
    "    for angle in np.arange(0, 2 * np.pi, 0.01):\n",
    "        value_vector = return_component_along_radius(component, mask, angle, center_x, center_y)\n",
    "        value_vector = value_vector[::-1]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(value_vector) and value_vector[i] == -1:\n",
    "            i += 1\n",
    "\n",
    "        value_vector = value_vector[i:]\n",
    "        value_matrix[:len(value_vector), idx] = value_vector\n",
    "        idx += 1\n",
    "\n",
    "    value_matrix[value_matrix == -1] = np.nan\n",
    "    median_curve = np.nanmedian(value_matrix, axis=1)\n",
    "\n",
    "    ###### For plotting image state before inversion\n",
    "    raw_value_matrix = -1 * np.ones((3601, 629))\n",
    "    idx = 0\n",
    "    for angle in np.arange(0, 2 * np.pi, 0.01):\n",
    "        value_vector = return_component_along_radius(component, mask, angle, center_x, center_y)\n",
    "    \n",
    "        i = 0\n",
    "        while i < len(value_vector) and value_vector[i] == -1:\n",
    "            i += 1\n",
    "    \n",
    "        value_vector_cleaned = value_vector[i:]\n",
    "        raw_value_matrix[:len(value_vector_cleaned), idx] = value_vector_cleaned\n",
    "        idx += 1\n",
    "\n",
    "    \n",
    "    # Compute derivative analysis\n",
    "    thickness_ranges = np.arange(20, 306, 5)\n",
    "    table_of_derivatives = np.zeros((500, len(thickness_ranges)))\n",
    "    \n",
    "    for i, t_range in enumerate(thickness_ranges):\n",
    "        x = np.arange(1, t_range + 1)\n",
    "        poly_fit = poly.Polynomial.fit(x, median_curve[:t_range], 5)\n",
    "        derivative_values = poly_fit.deriv()(x)\n",
    "        table_of_derivatives[:len(derivative_values), i] = derivative_values\n",
    "    \n",
    "    smoothed_mean_derivative = np.convolve(np.nanmean(table_of_derivatives, axis=1), np.ones(5)/5, mode='valid')\n",
    "    \n",
    "    \"\"\"Use variables below if using Satuation component of HSV.\"\"\"\n",
    "    #thickness = np.argmin(smoothed_mean_derivative)\n",
    "    #peak_derivative_value = np.min(smoothed_mean_derivative)\n",
    "    \n",
    "    \"\"\"Use variables below if using Value component of HSV.\"\"\"\n",
    "    thickness = np.argmax(smoothed_mean_derivative)\n",
    "    peak_derivative_value = np.max(smoothed_mean_derivative)\n",
    "\n",
    "\n",
    "    ### Fitted Polynomials Plot ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x = np.arange(len(median_curve))\n",
    "    plt.plot(median_curve, label='Median Curve', color='blue', alpha=0.4)\n",
    "    \n",
    "    for degree in [3, 5, 7]:\n",
    "        fit = poly.Polynomial.fit(x[:300], median_curve[:300], degree)\n",
    "        plt.plot(x[:300], fit(x[:300]), label=f'Degree {degree} Fit')\n",
    "    \n",
    "    plt.axvline(x=thickness, color='red', linestyle='--', label=f\"Estimated Thickness ({thickness})\")\n",
    "    plt.xlabel(\"Distance from outer edge\")\n",
    "    plt.ylabel(\"Component Value\")\n",
    "    plt.title(\"Polynomial Fits to Median Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "    \n",
    "    polyfit_plot_path = os.path.join(save_dir, f\"{os.path.basename(image_path).replace('.png', '')}_polyfit_curve.png\")\n",
    "    plt.savefig(polyfit_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    ### Pixel Sampling (Before Flip) ###\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(raw_value_matrix.T, cmap='gray', interpolation='nearest', aspect='auto', origin='upper')\n",
    "    plt.title(f\"Radial Sampling (Uninverted)  {os.path.basename(image_path)}\")\n",
    "    #plt.xlabel(\"Distance from Center (pixels)\")\n",
    "    #plt.ylabel(\"Angle Index\")\n",
    "    \n",
    "    # Limit x-axis to 1600\n",
    "    plt.xlim(0, 1100)\n",
    "     \n",
    "    raw_sampling_path = os.path.join(save_dir, f\"{os.path.basename(image_path).replace('.png', '')}_radial_sampling_unflipped.png\")\n",
    "    plt.savefig(raw_sampling_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ### Overlay Mask Center on Original Image ###\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(rgb_image)\n",
    "    ax.scatter(center_y, center_x, color='red', s=40, label='Center')  # note: (x, y) reversed in scatter\n",
    "    ax.set_title(f\"Mask Center on Original Image  {os.path.basename(image_path)}\")\n",
    "    ax.axis('off')\n",
    "    ax.legend()\n",
    "    \n",
    "    center_overlay_path = os.path.join(save_dir, f\"{os.path.basename(image_path).replace('.png', '')}_mask_center_overlay.png\")\n",
    "    plt.savefig(center_overlay_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    ### Save Median Curve Plot ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(median_curve, color=\"blue\", linestyle=\"dashed\", label=\"Median Curve\", alpha=0.6)\n",
    "    plt.axvline(x=thickness, color='red', linestyle='--', label=f'Thickness ({thickness})') ##########################\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Median over radii (HSV Value)\", fontsize=14)\n",
    "    plt.title(f\"Median Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    median_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_median_curve.png\"\n",
    "    median_plot_path = os.path.join(save_dir, median_plot_filename)\n",
    "    plt.savefig(median_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    ### Save Smoothed Derivative Curve ###\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(smoothed_mean_derivative, linewidth=2, color='red', label='Smoothed Mean Derivative')\n",
    "    plt.axvline(x=thickness, color='blue', linestyle='--', label=f'Thickness ({thickness})')\n",
    "    plt.xlabel(\"Distance from outer edge of bark\", fontsize=14)\n",
    "    plt.ylabel(\"Smoothed Mean Derivative\", fontsize=14)\n",
    "    plt.title(f\"Smoothed Derivative Curve for {os.path.basename(image_path)}\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    derivative_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_derivative_curve.png\"\n",
    "    derivative_plot_path = os.path.join(save_dir, derivative_plot_filename)\n",
    "    plt.savefig(derivative_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    ### Save Overlay of Thickness Estimate on Transformed Image ###\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot the transformed bark image (without exporting it separately)\n",
    "    ax1.imshow(value_matrix.T, cmap=\"gray\", interpolation=\"nearest\", aspect='auto')\n",
    "    ax1.set_title(f\"Transformed Image with Estimated Thickness ({os.path.basename(image_path)})\")\n",
    "    #ax1.set_xlabel(\"Angle Index\")\n",
    "    #ax1.set_ylabel(\"Distance from Outer Edge\")\n",
    "    ax1.set_xlim([0, 1100])  # Ensuring proper scaling\n",
    "\n",
    "    # Overlay only the estimated thickness line\n",
    "    ax1.axvline(x=thickness, color='red', linestyle='--', linewidth=2, label=f\"Estimated Thickness ({thickness})\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Save the overlay plot\n",
    "    overlay_plot_filename = f\"{os.path.basename(image_path).replace('.png', '')}_overlay_transformed.png\"\n",
    "    overlay_plot_path = os.path.join(save_dir, overlay_plot_filename)\n",
    "    plt.savefig(overlay_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return thickness, peak_derivative_value, os.path.basename(image_path), median_plot_path, derivative_plot_path, overlay_plot_path\n",
    "\n",
    "def return_component_along_radius(component, mask, angle, center_x, center_y):\n",
    "    width, height = component.shape\n",
    "    longest_radius = int(np.floor(np.sqrt(width**2 + height**2) / 2))\n",
    "    component_vector = -1 * np.ones(longest_radius + 1)\n",
    "\n",
    "    for r in range(longest_radius + 1):\n",
    "        x = int(center_x + r * np.cos(angle))\n",
    "        y = int(center_y + r * np.sin(angle))\n",
    "\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            if mask[x, y] == 255:\n",
    "                component_vector[r] = component[x, y]\n",
    "    \n",
    "    return component_vector\n",
    "\n",
    "def extract_id(filename):\n",
    "    match = re.match(r\"([A-Za-z0-9]+)-\", filename)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "def process_folder(input_folder, output_csv):\n",
    "    print(\" Starting bark thickness analysis...\")  # Start message\n",
    "    results_by_id = {}\n",
    "\n",
    "    for subdir, _, files in os.walk(input_folder):\n",
    "        if os.path.basename(subdir) != \"ends\":  \n",
    "            continue  # Skip folders that are not the target folder\n",
    "\n",
    "        png_files = [f for f in files if f.endswith(\".png\")]\n",
    "        if not png_files:\n",
    "            continue  # Skip if no PNG files are found in the target folder\n",
    "\n",
    "        parent_folder = os.path.dirname(subdir)  # Get the parent folder of 'target'\n",
    "        plot_folder = os.path.join(parent_folder, \"plots\")  # Save plots in the parent folder\n",
    "        os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n Processing target folder: {subdir} ({len(png_files)} images)\")\n",
    "\n",
    "        for filename in png_files:\n",
    "            image_path = os.path.join(subdir, filename)\n",
    "            print(f\"   Processing image: {filename}...\")\n",
    "\n",
    "            try:\n",
    "                thickness, peak_derivative_value, image_name, median_plot_path, derivative_plot_path, overlay_plot_path = compute_bark_thickness(image_path, plot_folder)\n",
    "\n",
    "                if thickness is not None:\n",
    "                    image_id = extract_id(filename)\n",
    "                    if image_id not in results_by_id:\n",
    "                        results_by_id[image_id] = []\n",
    "                    results_by_id[image_id].append((image_name, thickness))\n",
    "                    \n",
    "                    print(f\"     Thickness computed: {thickness} (Saved to {plot_folder})\")\n",
    "                else:\n",
    "                    print(f\"     Skipping {filename} (No valid thickness found)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"     Error processing {filename}: {e}\")\n",
    "\n",
    "    # Sort results by thickness\n",
    "    for image_id in results_by_id:\n",
    "        results_by_id[image_id].sort(key=lambda x: x[1])\n",
    "\n",
    "    # Write results to CSV\n",
    "    print(\"\\n Saving results to CSV...\")\n",
    "    write_header = not os.path.exists(output_csv) or os.stat(output_csv).st_size == 0  # Check if file is new or empty\n",
    "\n",
    "    with open(output_csv, \"a\") as f:\n",
    "        if write_header:\n",
    "            f.write(\"ID,file_name,thickness\\n\")  # Write header only once\n",
    "    \n",
    "        for image_id, images in results_by_id.items():\n",
    "            for image_name, thickness in images:\n",
    "                f.write(f\"{image_id},{image_name.replace('.png', '')},{thickness}\\n\")\n",
    "\n",
    "input_folder = 'bark_thickness/thesis_plots'\n",
    "output_csv = \"bark_thickness/thesis_plots/thesis_plot_max_v.csv\"\n",
    "\n",
    "# Process the folder\n",
    "process_folder(input_folder, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59434a1d-e3cb-4e7e-9397-b871f03c8752",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Merge Bark Thickness Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df1073-20ab-428d-ad46-0197243a242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "df1 = pd.read_csv('bark_thickness/contrast_test_extended_min_S.csv')  # Main dataset\n",
    "df2 = pd.read_csv('bark_thickness/contrast_test_extended_max_V.csv')  # Contains file_name and max_V\n",
    "\n",
    "# Drop duplicates or group by to ensure one max_V per file_name\n",
    "# Option 1: If duplicates are identical and safe to drop\n",
    "df2_unique = df2[['file_name', 'max_V']].drop_duplicates(subset='file_name')\n",
    "\n",
    "# Option 2: If file_name has multiple max_V values, keep the highest\n",
    "# df2_unique = df2.groupby('file_name', as_index=False)['max_V'].max()\n",
    "\n",
    "# Merge only on unique file_name values\n",
    "merged_df = pd.merge(df1, df2_unique, on='file_name', how='inner')\n",
    "\n",
    "# Save the result\n",
    "merged_df.to_csv('bark_thickness/merged_thickness_test_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984240e-8857-4a55-a748-c1e619914532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load merged dataset\n",
    "df = pd.read_csv('bark_thickness/merged_thickness_test_dataset.csv')\n",
    "\n",
    "# Sort by file_name if needed\n",
    "df = df.sort_values(by='file_name')\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot max_V\n",
    "plt.scatter(df['file_name'], df['max_V'], label='max_V', marker='o')\n",
    "\n",
    "# Plot min_S\n",
    "plt.scatter(df['file_name'], df['min_S'], label='min_S', marker='x')\n",
    "\n",
    "# Rotate x labels for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('File Name')\n",
    "plt.ylabel('Value')\n",
    "plt.title('max_V and min_S vs File Name')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
